# Crawl profile definitions for the Stage B crawler.
#
# Profiles describe how individual domains (or groups of domains) should be
# crawled when the scheduler dispatches work to the crawler agent. The default
# profile uses Crawl4AI to fetch the primary landing page for a site and rely on
# the existing extraction pipeline to generate structured articles. Specific
# domains can opt into deeper link traversal or alternative engines.
---
version: 1
defaults:
  profile: standard_crawl4ai
profiles:
  standard_crawl4ai:
    description: "Default Crawl4AI-assisted landing crawl"
    engine: crawl4ai
    mode: landing
    follow_internal_links: true
    max_pages: 50
    start_urls:
      - "https://{domain}"
    run_config:
      word_count_threshold: 60
      exclude_external_links: true
      remove_overlay_elements: true
      process_iframes: false
    link_preview:
      include_internal: true
      include_external: false
      max_links: 100
      concurrency: 3
  deep_financial_sections:
    description: "Deeper section crawl for complex financial publishers"
    engine: crawl4ai
    mode: section
    domains:
      - ft.com
      - wsj.com
    follow_internal_links: true
    max_pages: 8
    start_urls:
      - "https://{domain}/world/"
      - "https://{domain}/business/"
    run_config:
      word_count_threshold: 80
      exclude_external_links: true
      process_iframes: true
    link_preview:
      include_internal: true
      include_external: false
      max_links: 20
      concurrency: 4
      query: "top stories"
      score_threshold: 0.25
  legacy_generic:
    description: "Explicit opt-out of Crawl4AI for domains that rely on legacy logic"
    engine: generic
    domains:
      - reuters.com
      - apnews.com
