# vLLM Mistral-7B Configuration (with per-agent LoRA adapters)
# Used by agents to route inference to the vLLM endpoint instead of local transformers

model_family: mistral
model_size: 7B
quantization: none
context_length: 4096

# Endpoint (OpenAI-compatible)
endpoint:
  host: 127.0.0.1
  port: 7060
  base_url: http://127.0.0.1:7060/v1
  api_key: dummy  # vLLM doesn't require auth by default

# Per-agent adapter support (vLLM LoRA hot-swap)
adapters:
  enabled: true
  # Map agent name -> adapter path in model_store
  # Format: "agent_name:adapter_path" for --lora-modules
  synthesizer: /home/adra/JustNews/model_store/adapters/synthesizer/qwen2_synth_v1
  re_ranker: /home/adra/JustNews/model_store/adapters/re_ranker/qwen2_re_ranker_v1
  fact_checker: /home/adra/JustNews/model_store/adapters/fact_checker/qwen2_fact_checker_v1
  critic: /home/adra/JustNews/model_store/adapters/critic/qwen2_critic_v1
  journalist: /home/adra/JustNews/model_store/adapters/journalist/qwen2_journalist_v1
  chief_editor: /home/adra/JustNews/model_store/adapters/chief_editor/qwen2_chief_editor_v1
  reasoning: /home/adra/JustNews/model_store/adapters/reasoning/qwen2_reasoning_v1
  analyst: /home/adra/JustNews/model_store/adapters/analyst/qwen2_analyst_v1

# Training settings (QLoRA on 24GB 3090)
training:
  quantization: nf4
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  gradient_checkpointing: true
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-4
  warmup_steps: 100
  max_steps: 1000
  bf16: true
  optim: paged_adamw_8bit
  max_seq_length: 2048

# Fallback mode: if vLLM is unavailable, fall back to Mistral-7B + adapters
fallback:
  enabled: true
  model: mistralai/Mistral-7B-Instruct-v0.3
  adapters_base: /home/adra/JustNews/model_store/adapters
