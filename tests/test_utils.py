"""
Testing Utilities for JustNews

This module provides comprehensive testing utilities, helpers, and patterns
for the JustNews testing framework. It includes utilities for:

- Async testing patterns
- Mock creation and management
- Performance testing
- Data generation
- Test assertions
- Integration testing helpers

All utilities follow clean repository patterns and are designed for
production-ready testing infrastructure.
"""

import asyncio
import shutil
import tempfile
import time
from collections.abc import Callable
from contextlib import asynccontextmanager, contextmanager
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import pytest

# ============================================================================
# ASYNC TESTING UTILITIES
# ============================================================================

class AsyncTestHelper:
    """Helper class for async testing patterns"""

    @staticmethod
    async def wait_for_condition(
        condition_func: Callable[[], bool],
        timeout: float = 5.0,
        interval: float = 0.1
    ) -> bool:
        """Wait for a condition to become true"""
        start_time = asyncio.get_event_loop().time()
        while asyncio.get_event_loop().time() - start_time < timeout:
            if condition_func():
                return True
            await asyncio.sleep(interval)
        return False

    @staticmethod
    async def assert_eventually_true(
        condition_func: Callable[[], bool],
        timeout: float = 5.0,
        message: str = "Condition never became true"
    ):
        """Assert that a condition eventually becomes true"""
        result = await AsyncTestHelper.wait_for_condition(condition_func, timeout)
        assert result, message

    @staticmethod
    async def measure_async_execution_time(
        coro: Callable[[], Any]
    ) -> tuple[Any, float]:
        """Measure execution time of an async function"""
        start_time = asyncio.get_event_loop().time()
        result = await coro()
        end_time = asyncio.get_event_loop().time()
        return result, end_time - start_time


@asynccontextmanager
async def async_test_timeout(timeout_seconds: float = 30.0):
    """Async context manager for test timeouts"""
    try:
        yield await asyncio.wait_for(asyncio.sleep(0), timeout=timeout_seconds)
    except TimeoutError:
        pytest.fail(f"Test timed out after {timeout_seconds} seconds")


# ============================================================================
# MOCKING UTILITIES
# ============================================================================

class MockFactory:
    """Factory for creating comprehensive mocks"""

    @staticmethod
    def create_mock_agent(
        name: str,
        tools: list[str] | None = None,
        responses: dict[str, Any] | None = None
    ):
        """Create a mock agent for testing"""

        class MockAgent:
            def __init__(self):
                self.name = name
                self.tools = tools or ["analyze", "process"]
                self.responses = responses or {}
                self.call_history = []

            async def call_tool(self, tool: str, **kwargs):
                self.call_history.append({"tool": tool, "kwargs": kwargs})
                return self.responses.get(tool, {"status": "success", "data": {}})

            def get_call_history(self):
                return self.call_history.copy()

        return MockAgent()

    @staticmethod
    def create_mock_mcp_bus(agents: dict[str, str] | None = None):
        """Create a mock MCP Bus for testing"""

        class MockMCPBus:
            def __init__(self):
                self.agents = agents or {
                    "analyst": "http://localhost:8004",
                    "fact_checker": "http://localhost:8003",
                    "synthesizer": "http://localhost:8005"
                }
                self.calls = []

            def register_agent(self, registration: dict[str, Any]):
                """Register an agent with minimal validation for integration tests."""
                agent = registration.get("agent")
                address = registration.get("address") or f"http://localhost:{registration.get('port', 0)}"
                if not agent:
                    return False
                self.agents[agent] = address
                self.calls.append({
                    "action": "register_agent",
                    "agent": agent,
                    "address": address,
                    "timestamp": time.time()
                })
                return True

            async def call_agent(self, agent: str, tool: str, **kwargs):
                call_record = {
                    "agent": agent,
                    "tool": tool,
                    "kwargs": kwargs,
                    "timestamp": time.time()
                }
                self.calls.append(call_record)

                # Return realistic mock responses based on agent and tool
                if agent == "analyst" and tool == "analyze_sentiment":
                    return {
                        "status": "success",
                        "data": {
                            "sentiment": "neutral",
                            "confidence": 0.85,
                            "scores": {"positive": 0.3, "neutral": 0.5, "negative": 0.2},
                            "result": "mock_analyst_analyze_sentiment_result"
                        },
                        "processing_time": 0.1
                    }
                elif agent == "analyst" and tool == "extract_entities":
                    return {
                        "status": "success",
                        "data": {
                            "entities": ["technology", "advancements", "innovation"],
                            "confidence": 0.88,
                            "entity_types": ["NOUN", "NOUN", "NOUN"],
                            "result": "mock_analyst_extract_entities_result"
                        },
                        "processing_time": 0.12
                    }
                elif agent == "fact_checker" and tool == "verify_facts":
                    return {
                        "status": "success",
                        "data": {
                            "verdict": "verified",
                            "confidence": 0.92,
                            "sources_checked": 3,
                            "result": "mock_fact_checker_verify_facts_result"
                        },
                        "processing_time": 0.15
                    }
                elif agent == "fact_checker" and tool == "assess_credibility":
                    return {
                        "status": "success",
                        "data": {
                            "credibility_score": 0.85,
                            "rating": "high",
                            "factors": ["reputable_source", "fact_checking_history"],
                            "result": "mock_fact_checker_assess_credibility_result"
                        },
                        "processing_time": 0.08
                    }
                elif agent == "synthesizer" and tool == "synthesize_summary":
                    return {
                        "status": "success",
                        "data": {
                            "summary": "This is a mock synthesized summary of the provided articles.",
                            "word_count": 12,
                            "topics_covered": ["technology", "news"],
                            "result": "mock_synthesizer_synthesize_summary_result"
                        },
                        "processing_time": 0.2
                    }
                elif agent == "synthesizer" and tool == "extract_topics":
                    return {
                        "status": "success",
                        "data": {
                            "topics": ["technology", "innovation", "advancements"],
                            "topic_weights": [0.4, 0.3, 0.3],
                            "confidence": 0.76,
                            "result": "mock_synthesizer_extract_topics_result"
                        },
                        "processing_time": 0.18
                    }
                elif agent == "mcp_bus" and tool == "list_agents":
                    return {
                        "status": "success",
                        "data": {
                            "agents": ["analyst", "fact_checker", "synthesizer", "memory"],
                            "total_count": 4,
                            "active_count": 4
                        },
                        "processing_time": 0.05
                    }
                elif agent == "mcp_bus" and tool == "register_agent":
                    # Handle agent registration
                    name = kwargs.get("name")
                    address = kwargs.get("address")
                    if name and address:
                        self.agents[name] = address
                        return {
                            "status": "success",
                            "data": {
                                "registered": True,
                                "agent": name,
                                "address": address
                            },
                            "processing_time": 0.02
                        }
                    else:
                        return {
                            "status": "error",
                            "data": {"error": "Missing name or address"},
                            "processing_time": 0.01
                        }
                else:
                    # Generic response
                    return {
                        "status": "success",
                        "data": {"result": f"mock_{agent}_{tool}_result"},
                        "processing_time": 0.1
                    }

            def get_call_history(self):
                return self.calls.copy()

        return MockMCPBus()

    @staticmethod
    def create_mock_database():
        """Create a mock database for testing"""

        class MockDatabase:
            def __init__(self):
                self.data = {}
                self.connected = True

            async def connect(self):
                self.connected = True

            async def disconnect(self):
                self.connected = False

            async def execute(self, query: str, *args):
                # Simple mock execution
                if "INSERT" in query:
                    return 1
                elif "SELECT" in query:
                    return [{"id": 1, "content": "mock data"}]
                return 0

            async def fetch_one(self, query: str, *args):
                return {"id": 1, "content": "mock article", "meta": {}}

            async def fetch_all(self, query: str, *args):
                return [
                    {"id": 1, "content": "article 1", "meta": {}},
                    {"id": 2, "content": "article 2", "meta": {}}
                ]

    @staticmethod
    def create_mock_database_service():
        """Create a mock database service for testing"""

        class MockDatabaseService:
            def __init__(self):
                self.connected = True
                self.data = {}
                self.executed_queries: list[dict[str, Any]] = []

                class _Collection:
                    def __init__(self):
                        self.records: list[dict[str, Any]] = []

                    def add(self, **kwargs):
                        self.records.append(kwargs)
                        return None

                self.collection = _Collection()

            async def connect(self):
                self.connected = True
                return True

            async def disconnect(self):
                self.connected = False

            async def store_article(self, article_data: dict):
                article_id = article_data.get("id", f"mock_{len(self.data)}")
                self.data[article_id] = article_data
                return {"status": "success", "id": article_id}

            async def retrieve_article(self, article_id: str):
                return self.data.get(article_id, None)

            async def search_articles(self, query: str, limit: int = 10):
                # Simple mock search
                results = []
                for _article_id, article in self.data.items():
                    if query.lower() in article.get("content", "").lower():
                        results.append(article)
                        if len(results) >= limit:
                            break
                return results

            async def get_article_count(self):
                return len(self.data)

            def execute_query(self, query: str, params: Any | None = None):
                """Synchronous helper mirroring production execute_query signature."""
                record = {"query": query, "params": params}
                self.executed_queries.append(record)
                # Return mock primary key if INSERT detected to satisfy integration tests
                if isinstance(query, str) and query.strip().lower().startswith("insert"):
                    return {"article_id": len(self.executed_queries)}
                return {"status": "ok"}

        return MockDatabaseService()

class TestDataGenerator:
    """Generate test data for various scenarios"""

    @staticmethod
    def generate_articles(count: int = 5, sentiment: str = "neutral") -> list[dict]:
        """Generate sample articles for testing"""
        articles = []
        sentiments = {
            "positive": ["great news", "excellent results", "successful outcome"],
            "negative": ["concerning development", "serious issues", "problematic situation"],
            "neutral": ["news update", "information release", "current events"]
        }

        for i in range(count):
            content = f"This is {sentiments[sentiment][i % len(sentiments[sentiment])]} article number {i+1}."
            articles.append({
                "id": f"article-{i+1}",
                "content": content,
                "meta": {
                    "source": f"source-{i+1}",
                    "timestamp": f"2024-01-{i+1:02d}T00:00:00Z",
                    "sentiment": sentiment
                }
            })

        return articles

    @staticmethod
    def generate_mcp_payloads(count: int = 3) -> list[dict]:
        """Generate MCP call payloads for testing"""
        payloads = []
        agents = ["analyst", "fact_checker", "synthesizer"]
        tools = ["analyze_sentiment", "verify_facts", "synthesize_summary"]

        for i in range(count):
            payloads.append({
                "agent": agents[i % len(agents)],
                "tool": tools[i % len(tools)],
                "args": ["test content"],
                "kwargs": {"options": {"detailed": True}}
            })

        return payloads

    @staticmethod
    def generate_performance_data(samples: int = 100) -> list[float]:
        """Generate performance timing data"""
        import random
        return [random.uniform(0.001, 0.1) for _ in range(samples)]


# ============================================================================
# PERFORMANCE TESTING
# ============================================================================

@dataclass
class PerformanceMetrics:
    """Container for performance test results"""
    operation_name: str
    execution_times: list[float] = field(default_factory=list)
    memory_usage: list[float] = field(default_factory=list)
    success_rate: float = 1.0

    @property
    def average_time(self) -> float:
        return sum(self.execution_times) / len(self.execution_times) if self.execution_times else 0

    @property
    def median_time(self) -> float:
        sorted_times = sorted(self.execution_times)
        n = len(sorted_times)
        if n % 2 == 0:
            return (sorted_times[n//2 - 1] + sorted_times[n//2]) / 2
        return sorted_times[n//2]

    @property
    def p95_time(self) -> float:
        sorted_times = sorted(self.execution_times)
        if not sorted_times:
            return 0.0
        index = int(len(sorted_times) * 0.95)
        return sorted_times[min(index, len(sorted_times) - 1)]

    def assert_performance_requirements(
        self,
        max_average_time: float | None = None,
        max_p95_time: float | None = None,
        min_success_rate: float = 0.95
    ):
        """Assert that performance requirements are met"""
        if max_average_time:
            assert self.average_time <= max_average_time, \
                f"Average time {self.average_time:.3f}s exceeds limit {max_average_time}s"

        if max_p95_time:
            assert self.p95_time <= max_p95_time, \
                f"P95 time {self.p95_time:.3f}s exceeds limit {max_p95_time}s"

        assert self.success_rate >= min_success_rate, \
            f"Success rate {self.success_rate:.2%} below minimum {min_success_rate:.2%}"


class PerformanceTester:
    """Helper for performance testing"""

    def __init__(self, operation_name: str):
        self.operation_name = operation_name
        self.metrics = PerformanceMetrics(operation_name)

    async def measure_async_operation(
        self,
        operation: Callable[[], Any],
        iterations: int = 10
    ) -> PerformanceMetrics:
        """Measure performance of async operation"""
        import time

        for _ in range(iterations):
            start_time = time.perf_counter()
            try:
                await operation()
                end_time = time.perf_counter()
                self.metrics.execution_times.append(end_time - start_time)
            except Exception:
                # Track failures
                self.metrics.success_rate = (len(self.metrics.execution_times) / iterations)

        return self.metrics

    def measure_sync_operation(
        self,
        operation: Callable[[], Any],
        iterations: int = 10
    ) -> PerformanceMetrics:
        """Measure performance of sync operation"""
        import time

        for _ in range(iterations):
            start_time = time.perf_counter()
            try:
                operation()
                end_time = time.perf_counter()
                self.metrics.execution_times.append(end_time - start_time)
            except Exception:
                # Track failures
                self.metrics.success_rate = (len(self.metrics.execution_times) / iterations)

        return self.metrics

    def record_metric(self, name: str, value: float):
        """Record a custom metric"""
        if not hasattr(self.metrics, 'custom_metrics'):
            self.metrics.custom_metrics = {}
        if name not in self.metrics.custom_metrics:
            self.metrics.custom_metrics[name] = []
        self.metrics.custom_metrics[name].append(value)

@contextmanager
def temporary_directory():
    """Create a temporary directory for testing"""
    temp_dir = tempfile.mkdtemp()
    try:
        yield Path(temp_dir)
    finally:
        shutil.rmtree(temp_dir)


@contextmanager
def temporary_file(content: str = "", suffix: str = ".txt"):
    """Create a temporary file for testing"""
    with tempfile.NamedTemporaryFile(mode='w', suffix=suffix, delete=False) as f:
        f.write(content)
        temp_path = Path(f.name)

    try:
        yield temp_path
    finally:
        temp_path.unlink(missing_ok=True)


# ============================================================================
# ASSERTION HELPERS
# ============================================================================

class CustomAssertions:
    """Custom assertion helpers for domain-specific testing"""

    @staticmethod
    def assert_mcp_response_valid(response: dict):
        """Assert that MCP response has required fields"""
        required_fields = ["status", "data"]
        for field in required_fields:
            assert field in response, f"MCP response missing required field: {field}"

        assert response["status"] in ["success", "error"], \
            f"Invalid status: {response['status']}"

    @staticmethod
    def assert_article_structure(article: dict):
        """Assert that article has required structure"""
        required_fields = ["id", "content", "meta"]
        for field in required_fields:
            assert field in article, f"Article missing required field: {field}"

        assert isinstance(article["content"], str), "Article content must be string"
        assert isinstance(article["meta"], dict), "Article meta must be dict"

    @staticmethod
    def assert_agent_call_recorded(
        mock_bus,
        agent: str,
        tool: str,
        expected_calls: int = 1
    ):
        """Assert that agent call was recorded"""
        calls = [call for call in mock_bus.calls
                if call["agent"] == agent and call["tool"] == tool]
        assert len(calls) == expected_calls, \
            f"Expected {expected_calls} calls to {agent}.{tool}, got {len(calls)}"

    @staticmethod
    def assert_performance_within_limits(
        execution_time: float,
        max_time: float,
        operation_name: str = "operation"
    ):
        """Assert that execution time is within limits"""
        assert execution_time <= max_time, \
            f"{operation_name} took {execution_time:.3f}s, limit was {max_time}s"

    @staticmethod
    def assert_valid_agent_registration(registration_data: dict):
        """Assert that agent registration data is valid"""
        required_fields = ["agent", "port", "capabilities"]
        for field in required_fields:
            assert field in registration_data, f"Registration missing required field: {field}"

        assert isinstance(registration_data["port"], int), "Port must be integer"
        assert isinstance(registration_data["capabilities"], list), "Capabilities must be list"
        assert len(registration_data["capabilities"]) > 0, "Must have at least one capability"

    @staticmethod
    def assert_valid_news_processing_result(result: dict[str, Any]):
        """Validate final news processing payload used in integration tests."""
        required_fields = {
            "article_id": int,
            "title": str,
            "summary": str,
            "sentiment": str,
            "fact_check_verdict": str,
            "source_credibility": (int, float),
            "processing_time": (int, float),
        }

        for field, expected_type in required_fields.items():
            assert field in result, f"Result missing required field: {field}"
            assert isinstance(result[field], expected_type), \
                f"Field {field} expected type {expected_type}, got {type(result[field])}"

        assert result["summary"].strip(), "Summary cannot be empty"
        assert result["processing_time"] >= 0, "Processing time must be non-negative"


# ============================================================================
# INTEGRATION TESTING HELPERS
# ============================================================================

class IntegrationTestHelper:
    """Helpers for integration testing"""

    @staticmethod
    @asynccontextmanager
    async def setup_test_services():
        """Setup test services for integration testing"""
        # This would start mock services, databases, etc.
        # For now, just yield control
        yield

    @staticmethod
    async def wait_for_service_ready(
        service_url: str,
        timeout: float = 10.0
    ) -> bool:
        """Wait for a service to be ready"""
        import aiohttp

        async with aiohttp.ClientSession() as session:
            start_time = asyncio.get_event_loop().time()
            while asyncio.get_event_loop().time() - start_time < timeout:
                try:
                    async with session.get(f"{service_url}/health") as response:
                        if response.status == 200:
                            return True
                except Exception:
                    pass
                await asyncio.sleep(0.5)
        return False

    @staticmethod
    async def cleanup_test_data():
        """Clean up test data after integration tests"""
        # Implementation would depend on the specific test data
        pass


# ============================================================================
# TEST CONFIGURATION
# ============================================================================

class TestConfig:
    """Centralized test configuration"""

    # Timeouts
    DEFAULT_TIMEOUT = 30.0
    ASYNC_TIMEOUT = 10.0
    SERVICE_READY_TIMEOUT = 15.0

    # Performance thresholds
    MAX_RESPONSE_TIME = 1.0  # seconds
    MAX_PROCESSING_TIME = 5.0  # seconds

    # Test data sizes
    SMALL_TEST_SIZE = 10
    MEDIUM_TEST_SIZE = 100
    LARGE_TEST_SIZE = 1000

    # Environment variables
    GPU_AVAILABLE = "TEST_GPU_AVAILABLE"
    DATABASE_URL = "TEST_DATABASE_URL"
    MCP_BUS_URL = "TEST_MCP_BUS_URL"

    @classmethod
    def get_timeout(cls, test_type: str = "default") -> float:
        """Get timeout for test type"""
        timeouts = {
            "async": cls.ASYNC_TIMEOUT,
            "service": cls.SERVICE_READY_TIMEOUT,
            "default": cls.DEFAULT_TIMEOUT
        }
        return timeouts.get(test_type, cls.DEFAULT_TIMEOUT)

    @classmethod
    def is_gpu_available(cls) -> bool:
        """Check if GPU is available for testing"""
        import os
        return os.environ.get(cls.GPU_AVAILABLE, "false").lower() == "true"
