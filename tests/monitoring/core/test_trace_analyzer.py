import pytest
from unittest.mock import MagicMock, patch
from datetime import datetime, timedelta, timezone
from dataclasses import asdict

from monitoring.core.trace_analyzer import (
    TraceAnalyzer, 
    AnomalyType, 
    AnomalyAlert,
    ServiceHealthScore,
    TrendAnalysis
)
from monitoring.core.trace_processor import TraceAnalysis, ServiceDependency
from monitoring.core.trace_collector import TraceData, TraceSpan

@pytest.fixture
def analyzer():
    return TraceAnalyzer(analysis_window_minutes=60, anomaly_sensitivity=0.8)

@pytest.fixture
def sample_trace_span():
    def _create_span(
        span_id="span-1",
        service_name="test-service",
        operation="test-op",
        duration_ms=100.0,
        status="ok",
        start_time=None
    ):
        if start_time is None:
            start_time = datetime.now(timezone.utc)
        
        return TraceSpan(
            trace_id="trace-1",
            span_id=span_id,
            parent_span_id=None,
            name=operation,
            kind="INTERNAL",
            service_name=service_name,
            operation=operation,
            start_time=start_time,
            end_time=start_time + timedelta(milliseconds=duration_ms),
            duration_ms=duration_ms,
            status=status,
            attributes={},
            events=[]
        )
    return _create_span

@pytest.fixture
def sample_trace_data(sample_trace_span):
    def _create_trace(
        trace_id="trace-1",
        spans=None,
        duration_ms=200.0,
        service_count=1
    ):
        if spans is None:
            spans = [sample_trace_span()]
            
        start_time = datetime.now(timezone.utc)
        return TraceData(
            trace_id=trace_id,
            root_span_id=spans[0].span_id if spans else "root",
            spans=spans,
            start_time=start_time,
            end_time=start_time + timedelta(milliseconds=duration_ms),
            duration_ms=duration_ms,
            service_count=service_count,
            total_spans=len(spans),
            error_count=sum(1 for s in spans if s.status != "ok"),
            status="active"
        )
    return _create_trace

@pytest.fixture
def sample_trace_analysis():
    return TraceAnalysis(
        trace_id="trace-1",
        total_duration_ms=200.0,
        span_count=1,
        service_count=1,
        error_count=0,
        critical_path=["span-1"],
        bottlenecks=[],
        service_dependencies=[],
        recommendations=[]
    )

class TestTraceAnalyzer:
    
    def test_initialization(self, analyzer):
        assert analyzer.analysis_window_minutes == 60
        assert len(analyzer.recent_traces) == 0
        assert len(analyzer.performance_baselines) == 0

    def test_update_baselines(self, analyzer, sample_trace_data, sample_trace_span):
        # Create history of traces
        spans = [sample_trace_span(duration_ms=100.0)]
        trace1 = sample_trace_data(trace_id="t1", spans=spans, duration_ms=100.0)
        
        spans2 = [sample_trace_span(duration_ms=200.0)]
        trace2 = sample_trace_data(trace_id="t2", spans=spans2, duration_ms=200.0)
        
        # Inject directly into recent_traces
        analyzer.recent_traces = [trace1, trace2]
        
        analyzer.update_baselines()
        
        # Check global duration baseline
        # Mean of 100, 200 = 150
        baseline = analyzer.performance_baselines.get("global:duration")
        assert baseline is not None
        assert baseline["mean"] == 150.0
        
        # Check service latency baseline
        service_baseline = analyzer.performance_baselines.get("test-service:latency")
        assert service_baseline is not None
        assert service_baseline["mean"] == 150.0

    def test_detect_latency_anomalies(self, analyzer, sample_trace_data, sample_trace_span):
        # Setup baseline
        analyzer.performance_baselines["test-service:test-op"] = {
            "mean": 100.0,
            "std": 10.0
        }
        
        # Normal trace (z-score 0)
        spans_normal = [sample_trace_span(duration_ms=100.0)]
        trace_normal = sample_trace_data(spans=spans_normal)
        anomalies = analyzer._detect_latency_anomalies(trace_normal)
        assert len(anomalies) == 0
        
        # Anomalous trace (z-score > 3) -> 100 + (3.1 * 10) = 131
        spans_spike = [sample_trace_span(duration_ms=150.0)]
        trace_spike = sample_trace_data(spans=spans_spike)
        anomalies = analyzer._detect_latency_anomalies(trace_spike)
        
        assert len(anomalies) == 1
        assert anomalies[0].anomaly_type == AnomalyType.LATENCY_SPIKE
        assert anomalies[0].severity == "high" # z-score 5.0

    def test_detect_error_anomalies(self, analyzer, sample_trace_data, sample_trace_span):
        # Baseline very low error rate
        analyzer.performance_baselines["test-service:error_rate"] = {
            "mean": 0.01
        }
        
        # Trace with high error rate (100% errors)
        spans = [
            sample_trace_span(status="error"),
            sample_trace_span(status="error"),
            sample_trace_span(status="error"),
            sample_trace_span(status="error")
        ]
        trace = sample_trace_data(spans=spans)
        
        anomalies = analyzer._detect_error_anomalies(trace)
        assert len(anomalies) == 1
        assert anomalies[0].anomaly_type == AnomalyType.ERROR_RATE_SPIKE
        assert anomalies[0].severity == "critical" # > 20%
        assert anomalies[0].affected_services == ["test-service"]

    def test_detect_pattern_anomalies(self, analyzer, sample_trace_data, sample_trace_span, sample_trace_analysis):
        # Baseline span count 10
        analyzer.performance_baselines["global:span_count"] = {"mean": 10.0}
        
        # Trace with 20 spans (100% change, > 50% threshold)
        spans = [sample_trace_span()] * 20
        trace = sample_trace_data(spans=spans)
        
        anomalies = analyzer._detect_pattern_anomalies(trace, sample_trace_analysis)
        
        # Should detect span count anomaly
        types = [a.anomaly_type for a in anomalies]
        assert AnomalyType.UNUSUAL_PATTERN in types

    def test_detect_dependency_anomalies(self, analyzer, sample_trace_data, sample_trace_analysis):
        # Configure analysis with failed dependency
        dep = ServiceDependency(
            source_service="service-a",
            target_service="service-b",
            operation="call",
            error_rate=0.5 # 50% failure, > 10% threshold
        )
        sample_trace_analysis.service_dependencies = [dep]
        
        anomalies = analyzer._detect_dependency_anomalies(sample_trace_analysis)
        
        assert len(anomalies) == 1
        assert anomalies[0].anomaly_type == AnomalyType.DEPENDENCY_FAILURE
        assert "service-a" in anomalies[0].affected_services
        assert "service-b" in anomalies[0].affected_services

    def test_analyze_trace_integration(self, analyzer, sample_trace_data, sample_trace_analysis):
        # Test the main entry point
        trace = sample_trace_data()
        
        # Should return list (empty or not depending on defaults)
        result = analyzer.analyze_trace(trace, sample_trace_analysis)
        assert isinstance(result, list)
        
        # Check trace was added to history
        assert len(analyzer.recent_traces) == 1
        assert analyzer.recent_traces[0].trace_id == trace.trace_id

    def test_service_health_scoring(self, analyzer, sample_trace_data, sample_trace_span, sample_trace_analysis):
        # Create a trace with some characteristics
        spans = [
            sample_trace_span(duration_ms=100.0, status="ok"),
            sample_trace_span(duration_ms=100.0, status="error")
        ]
        trace = sample_trace_data(spans=spans)
        
        analyzer._update_service_health_scores(trace, sample_trace_analysis)
        
        score = analyzer.service_health_scores.get("test-service")
        assert score is not None
        assert isinstance(score, ServiceHealthScore)
        # Error score should be penalized (50% error rate)
        # Formula: max(0, 100 - (error_rate * 1000)) -> 100 - 500 = 0?
        # Let's check the formula in source: max(0, 100 - (error_rate * 1000))
        # 0.5 * 1000 = 500. 100 - 500 = -400. Max(0, -400) = 0.
        assert score.error_score == 0

    def test_analyze_trends(self, analyzer, sample_trace_span, sample_trace_data):
        # Add a series of traces over time to simulate a trend
        now = datetime.now(timezone.utc)
        
        traces = []
        for i in range(10):
            # Increasing duration: 100, 110, 120...
            t_time = now - timedelta(minutes=10 - i)
            span = sample_trace_span(duration_ms=100 + i * 10, start_time=t_time)
            
            # Create trace data manually to set end_time correctly for trend window check
            trace = TraceData(
                trace_id=f"t{i}",
                root_span_id="s1",
                spans=[span],
                start_time=t_time,
                end_time=t_time + timedelta(milliseconds=100),
                duration_ms=100 + i * 10,
                status="active"
            )
            traces.append(trace)
            
        analyzer.recent_traces = traces
        
        # There should be a degrading trend
        # Note: analyze_trends filters by time_window. 
        # "medium" window is 2 hours. Our traces are within last 10 mins.
        
        start_time_naive = datetime.now() # This is what analyzer uses internally currently
        # Wait, if I mock recent_traces with aware datetimes, and analyzer compares with naive, 
        # it might crash or filter incorrectly.
        
        # Let's try running it. If it fails due to TZ, I'll fix the source.
        # But wait, trace_analyzer.py uses:
        # cutoff_time = datetime.now() - window_duration
        # trace.end_time < cutoff_time
        
        # If trace.end_time is aware (from my fixture), and datetime.now() is naive, this bursts.
        # I suspect I WILL need to fix source code.
        
        # Skipping assertion of specific result, focusing on ensuring it runs or identifying the crash.
        try:
            trends = analyzer.analyze_trends(service_name="test-service")
            # If it works, great.
        except TypeError:
            # Expected if TZ mismatch
            pytest.fail("Timezone mismatch in analyze_trends")

    def test_deduplicate_anomalies(self, analyzer):
        a1 = AnomalyAlert(
            anomaly_id="1", anomaly_type=AnomalyType.LATENCY_SPIKE, 
            severity="high", description="desc", affected_services=["s1"], 
            evidence={}, recommendations=[]
        )
        a2 = AnomalyAlert(
            anomaly_id="2", anomaly_type=AnomalyType.LATENCY_SPIKE, # Same Type
            severity="high", description="desc", affected_services=["s1"], # Same Service
            evidence={}, recommendations=[]
        )
        
        deduped = analyzer._deduplicate_anomalies([a1, a2])
        assert len(deduped) == 1
