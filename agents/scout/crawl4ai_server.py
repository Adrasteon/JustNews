"""Deprecated shim for scout-local Crawl4AI server.

The canonical Crawl4AI bridge has moved to `agents.c4ai`. This module
remains as a tiny compatibility shim that emits a DeprecationWarning on
import and re-exports the canonical objects so third-party imports keep
working until callers are migrated.

Remove this shim once all internal imports reference `agents.c4ai`.
"""
from warnings import warn

warn(
    "agents.c4ai.server is deprecated; import agents.c4ai.server instead",
    DeprecationWarning,
)

# Try to re-export the canonical server API. If the canonical module is
# not importable, provide a very small fallback so health checks don't hard
# fail during transitional deployments.
try:
    from agents.c4ai.server import app, CrawlRequest, crawl, health  # type: ignore
    __all__ = ["app", "CrawlRequest", "crawl", "health"]
except Exception:  # pragma: no cover - fallback path
    from fastapi import FastAPI

    app = FastAPI(title="deprecated-crawl4ai-bridge")

    @app.get("/health")
    async def health() -> dict:
        return {"status": "deprecated", "service": "crawl4ai-bridge"}

    __all__ = ["app", "health"]
"""Deprecated shim for scout-local Crawl4AI server.

The canonical Crawl4AI bridge has moved to `agents.c4ai`. This module
remains as a tiny compatibility shim that emits a DeprecationWarning on
import and re-exports the canonical objects so third-party imports keep
working until callers are migrated.

Remove this shim once all internal imports reference `agents.c4ai`.
"""
from warnings import warn

warn(
    "agents.c4ai.server is deprecated; import agents.c4ai.server instead",
    DeprecationWarning,
)

# Try to re-export the canonical server API. If the canonical module is
# not importable, provide a very small fallback so health checks don't hard
# fail during transitional deployments.
try:
    from agents.c4ai.server import app, CrawlRequest, crawl, health  # type: ignore
    __all__ = ["app", "CrawlRequest", "crawl", "health"]
except Exception:
    from fastapi import FastAPI

    app = FastAPI(title="deprecated-crawl4ai-bridge")

    @app.get("/health")
    async def health() -> dict:  # pragma: no cover - fallback
        return {"status": "deprecated", "service": "crawl4ai-bridge"}

    __all__ = ["app", "health"]
"""Crawl4AI bridge HTTP server.

Small FastAPI wrapper that lazily imports crawl4ai and optional crawler
enhancements. The server applies best-effort wiring for UA/proxy/stealth,
robots and rate limiting, modal removal and paywall detection. Optional
components are ignored if unavailable so the service remains usable.
"""
from __future__ import annotations

import os
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="JustNewsAgent Crawl4AI Bridge")


class CrawlRequest(BaseModel):
    urls: List[str]
    mode: str = "standard"
    use_llm: bool = True


def _lazy_import(path: str, names: Optional[List[str]] = None) -> Optional[Tuple]:
    try:
        if names:
            mod = __import__(path, fromlist=names)
            return tuple(getattr(mod, n) for n in names)
        __import__(path)
        return (True,)
    except Exception:
        return None


@app.get("/health")
async def health() -> Dict[str, Any]:
    return {"status": "ok", "service": "crawl4ai-bridge"}


@app.post("/crawl")
async def crawl(req: CrawlRequest) -> Dict[str, Any]:
    # core dependency
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode  # type: ignore
    except Exception as exc:
        raise HTTPException(status_code=503, detail=f"crawl4ai not available: {exc}")

    # optional helpers (best-effort)
    modal_t = _lazy_import("agents.crawler.enhancements.modal_handler", ["ModalHandler"]) or (None,)
    ModalHandler = modal_t[0]
    paywall_t = _lazy_import("agents.crawler.enhancements.paywall_detector", ["PaywallDetector"]) or (None,)
    PaywallDetector = paywall_t[0]
    cu_t = _lazy_import("agents.crawler.crawler_utils", ["record_paywall_detection", "RobotsChecker", "RateLimiter"]) or (None, None, None)
    record_paywall_detection, RobotsChecker, RateLimiter = cu_t
    stealth_t = _lazy_import("agents.crawler.enhancements.stealth_browser", ["StealthBrowserFactory"]) or (None,)
    StealthBrowserFactory = stealth_t[0]
    ua_t = _lazy_import("agents.crawler.enhancements.ua_rotation", ["UserAgentProvider", "UserAgentConfig"]) or (None, None)
    UserAgentProvider, UserAgentConfig = ua_t
    proxy_t = _lazy_import("agents.crawler.enhancements.proxy_manager", ["ProxyManager", "PIASocks5Manager", "ProxyDefinition"]) or (None, None, None)
    ProxyManager, PIASocks5Manager, ProxyDefinition = proxy_t

    # runtime flags
    persist_paywalls = os.getenv("CRAWL4AI_PERSIST_PAYWALLS", "true").lower() in ("1", "true", "yes")
    paywall_threshold = int(os.getenv("UNIFIED_CRAWLER_PAYWALL_SKIP_THRESHOLD", "3"))

    # instantiate helpers
    modal_handler = ModalHandler() if ModalHandler is not None else None
    paywall_detector = PaywallDetector() if PaywallDetector is not None else None
    robots_checker = RobotsChecker() if RobotsChecker is not None else None
    rate_limiter = RateLimiter() if RateLimiter is not None else None

    stealth_factory = None
    try:
        if StealthBrowserFactory is not None:
            stealth_factory = StealthBrowserFactory()
    except Exception:
        stealth_factory = None

    ua_provider = None
    try:
        if UserAgentProvider is not None and UserAgentConfig is not None:
            ua_pool = None
            if stealth_factory is not None:
                try:
                    ua_pool = [getattr(p, "user_agent", None) for p in getattr(stealth_factory, "_profiles", [])]
                except Exception:
                    ua_pool = None
            ua_config = UserAgentConfig(pool=ua_pool or [], per_domain_overrides={}, default=(ua_pool[0] if ua_pool else None))
            ua_provider = UserAgentProvider(ua_config)
    except Exception:
        ua_provider = None

    proxy_manager = None
    try:
        if PIASocks5Manager is not None and getattr(PIASocks5Manager, "is_available", lambda: False)():
            proxy_manager = PIASocks5Manager()
        elif ProxyManager is not None:
            env_pool = os.getenv("CRAWL4AI_PROXY_POOL")
            if env_pool:
                try:
                    proxies = [ProxyDefinition(url=p.strip()) for p in env_pool.split(",") if p.strip()]
                    proxy_manager = ProxyManager(proxies)
                except Exception:
                    proxy_manager = ProxyManager()
            else:
                proxy_manager = ProxyManager()
    except Exception:
        proxy_manager = None

    results: List[Dict[str, Any]] = []

    # build browser config
    browser_cfg = None
    try:
        browser_kwargs: Dict[str, Any] = {"headless": True}
        try:
            if ua_provider is not None:
                browser_kwargs["user_agent"] = ua_provider.choose(domain=None)
            elif stealth_factory is not None:
                prof = stealth_factory.random_profile()
                browser_kwargs["user_agent"] = getattr(prof, "user_agent", None)
        except Exception:
            pass

        try:
            if proxy_manager is not None:
                proxy_def = None
                if hasattr(proxy_manager, "next_proxy"):
                    proxy_def = proxy_manager.next_proxy()
                elif hasattr(proxy_manager, "get_proxy"):
                    proxy_def = proxy_manager.get_proxy()
                if proxy_def is not None and getattr(proxy_def, "url", None):
                    browser_kwargs["proxy"] = proxy_def.url
        except Exception:
            pass

        try:
            browser_cfg = BrowserConfig(**browser_kwargs)
        except Exception:
            try:
                browser_cfg = BrowserConfig(headless=True)
                for k, v in browser_kwargs.items():
                    try:
                        setattr(browser_cfg, k, v)
                    except Exception:
                        pass
            except Exception:
                browser_cfg = None
    except Exception:
        browser_cfg = None

    if browser_cfg is None:
        raise HTTPException(status_code=500, detail="unable to construct browser configuration")

    # run crawler
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        for url in req.urls:
            # robots check
            if robots_checker is not None:
                try:
                    if not robots_checker.is_allowed(url):
                        """Deprecated wrapper: re-export canonical crawl4ai server.

                        This module used to contain a scout-specific Crawl4AI server. The
                        implementation has been consolidated under `agents.c4ai.server` and this
                        module now re-exports the canonical objects so older imports keep
                        working while avoiding duplicate code paths.

                        Keep this shim until callers are updated to import `agents.c4ai.server`.
                        """
                        from warnings import warn

                        warn("agents.c4ai.server is deprecated; import agents.c4ai.server instead", DeprecationWarning)

                        # Re-export the canonical FastAPI app and helpers
                        try:
                            from agents.c4ai.server import app, CrawlRequest, crawl, health  # type: ignore
                        except Exception:
                            # If the canonical server isn't importable, keep a minimal fallback app
                            from fastapi import FastAPI

                            app = FastAPI(title="deprecated-crawl4ai-bridge")

                            @app.get("/health")
                            async def health() -> dict:
                                return {"status": "deprecated", "service": "crawl4ai-bridge"}
                prof = stealth_factory.random_profile()
                browser_kwargs["user_agent"] = getattr(prof, "user_agent", None)
        except Exception:
            pass

        try:
            if proxy_manager is not None:
                proxy_def = None
                if hasattr(proxy_manager, "next_proxy"):
                    proxy_def = proxy_manager.next_proxy()
                elif hasattr(proxy_manager, "get_proxy"):
                    proxy_def = proxy_manager.get_proxy()
                if proxy_def is not None and getattr(proxy_def, "url", None):
                    browser_kwargs["proxy"] = proxy_def.url
        except Exception:
            pass

        try:
            browser_cfg = BrowserConfig(**browser_kwargs)
        except Exception:
            try:
                browser_cfg = BrowserConfig(headless=True)
                for k, v in browser_kwargs.items():
                    try:
                        setattr(browser_cfg, k, v)
                    except Exception:
                        pass
            except Exception:
                browser_cfg = None
    except Exception:
        browser_cfg = None

    if browser_cfg is None:
        raise HTTPException(status_code=500, detail="unable to construct browser configuration")

    # Run the crawler and apply enhancements per-URL
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        for url in req.urls:
            # Robots check
            if robots_checker is not None:
                try:
                    if not robots_checker.is_allowed(url):
                        results.append({"url": url, "error": "disallowed_by_robots", "success": False})
                        continue
                except Exception:
                    pass

            # Rate limiting per-domain
            if rate_limiter is not None:
                try:
                    domain = urlparse(url).netloc
                    rate_limiter.acquire(domain)
                except Exception:
                    pass

            try:
                try:
                    run_conf = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
                    res = await crawler.arun(url, config=run_conf)
                except Exception:
                    res = await crawler.arun(url)

                url_val = getattr(res, "url", url)
                html = getattr(res, "html", None) or ""
                markdown = getattr(res, "markdown", None)

                # Modal handling
                modal_result = None
                if modal_handler and html:
                    try:
                        modal_result = modal_handler.process(html)
                        html = getattr(modal_result, "cleaned_html", html)
                    except Exception:
                        modal_result = None

                # Paywall detection
                paywall_result = None
                skip_ingest = False
                if paywall_detector:
                    try:
                        paywall_result = await paywall_detector.analyze(url=url_val, html=html, text=markdown)
                        skip_ingest = bool(getattr(paywall_result, "should_skip", False))
                    except Exception:
                        paywall_result = None

                if skip_ingest and persist_paywalls and record_paywall_detection:
                    try:
                        domain = urlparse(url_val).netloc
                        record_paywall_detection(source_id=None, domain=domain, skip_count=1, threshold=paywall_threshold, paywall_type="hard")
                    except Exception:
                        pass

                entry: Dict[str, Any] = {
                    "url": url_val,
                    "title": getattr(res, "title", None),
                    "html": html,
                    "markdown": markdown,
                    "links": getattr(res, "links", []),
                    "status_code": getattr(res, "status_code", None),
                    "success": getattr(res, "success", True),
                    "skip_ingest": skip_ingest,
                }

                if modal_result is not None:
                    entry["modal"] = {
                        "modals_detected": getattr(modal_result, "modals_detected", False),
                        "applied_cookies": getattr(modal_result, "applied_cookies", {}),
                        "notes": getattr(modal_result, "notes", []),
                    }

                if paywall_result is not None:
                    entry["paywall"] = {
                        "is_paywall": getattr(paywall_result, "is_paywall", False),
                        "confidence": float(getattr(paywall_result, "confidence", 0.0)),
                        "reasons": getattr(paywall_result, "reasons", []),
                        "metadata": getattr(paywall_result, "metadata", {}),
                    }

                results.append(entry)
            except Exception as exc:
                results.append({"url": url, "error": str(exc), "success": False})

    return {"results": results}
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="JustNewsAgent Crawl4AI Bridge")


class CrawlRequest(BaseModel):
    urls: List[str]
    mode: str = "standard"
    use_llm: bool = True


@app.get("/health")
async def health() -> Dict[str, Any]:
    return {"status": "ok", "service": "crawl4ai-bridge"}


@app.post("/crawl")
async def crawl(req: CrawlRequest) -> Dict[str, Any]:
    # Lazy imports for heavy/optional dependencies
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    except Exception as exc:  # pragma: no cover - may be missing in some test environments
        raise HTTPException(status_code=503, detail=f"crawl4ai not available: {exc}")

    # Optional helpers: best-effort import, default to None on failure
    def _import(path: str, names: Optional[List[str]] = None):
        try:
            if names:
                mod = __import__(path, fromlist=names)
                return tuple(getattr(mod, n) for n in names)
            return __import__(path)
        except Exception:
            return None

    ModalHandler = _import("agents.crawler.enhancements.modal_handler", ["ModalHandler"]) or (None,)
    ModalHandler = ModalHandler[0] if isinstance(ModalHandler, tuple) else None
    PaywallDetector = _import("agents.crawler.enhancements.paywall_detector", ["PaywallDetector"]) or (None,)
    PaywallDetector = PaywallDetector[0] if isinstance(PaywallDetector, tuple) else None

    cu = _import("agents.crawler.crawler_utils", ["record_paywall_detection", "RobotsChecker", "RateLimiter"]) or (None, None, None)
    record_paywall_detection, RobotsChecker, RateLimiter = cu if isinstance(cu, tuple) else (None, None, None)

    sf = _import("agents.crawler.enhancements.stealth_browser", ["StealthBrowserFactory"]) or (None,)
    StealthBrowserFactory = sf[0] if isinstance(sf, tuple) else None

    ua = _import("agents.crawler.enhancements.ua_rotation", ["UserAgentProvider", "UserAgentConfig"]) or (None, None)
    UserAgentProvider, UserAgentConfig = ua if isinstance(ua, tuple) else (None, None)

    pm = _import("agents.crawler.enhancements.proxy_manager", ["ProxyManager", "PIASocks5Manager", "ProxyDefinition"]) or (None, None, None)
    ProxyManager, PIASocks5Manager, ProxyDefinition = pm if isinstance(pm, tuple) else (None, None, None)

    # Runtime flags
    persist_paywalls = os.getenv("CRAWL4AI_PERSIST_PAYWALLS", "true").lower() in ("1", "true", "yes")
    paywall_threshold = int(os.getenv("UNIFIED_CRAWLER_PAYWALL_SKIP_THRESHOLD", "3"))

    # Instantiate helpers if available (best-effort)
    modal_handler = ModalHandler() if ModalHandler is not None else None
    paywall_detector = PaywallDetector() if PaywallDetector is not None else None
    robots_checker = RobotsChecker() if RobotsChecker is not None else None
    rate_limiter = RateLimiter() if RateLimiter is not None else None

    stealth_factory = None
    try:
        if StealthBrowserFactory is not None:
            stealth_factory = StealthBrowserFactory()
    except Exception:
        stealth_factory = None

    ua_provider = None
    try:
        if UserAgentProvider is not None and UserAgentConfig is not None:
            ua_pool = None
            if stealth_factory is not None:
                try:
                    ua_pool = [p.user_agent for p in getattr(stealth_factory, "_profiles", [])]
                except Exception:
                    ua_pool = None
            ua_config = UserAgentConfig(pool=ua_pool or [], per_domain_overrides={}, default=(ua_pool[0] if ua_pool else None))
            ua_provider = UserAgentProvider(ua_config)
    except Exception:
        ua_provider = None

    proxy_manager = None
    try:
        if PIASocks5Manager is not None and getattr(PIASocks5Manager, "is_available", lambda: False)():
            proxy_manager = PIASocks5Manager()
        elif ProxyManager is not None:
            env_pool = os.getenv("CRAWL4AI_PROXY_POOL")
            if env_pool:
                try:
                    proxies = [ProxyDefinition(url=p.strip()) for p in env_pool.split(",") if p.strip()]
                    proxy_manager = ProxyManager(proxies)
                except Exception:
                    proxy_manager = ProxyManager()
            else:
                proxy_manager = ProxyManager()
    except Exception:
        proxy_manager = None

    results: List[Dict[str, Any]] = []

    # Build BrowserConfig with best-effort UA/proxy settings
    browser_cfg = None
    try:
        browser_kwargs: Dict[str, Any] = {"headless": True}
        try:
            if ua_provider is not None:
                browser_kwargs["user_agent"] = ua_provider.choose(domain=None)
            elif stealth_factory is not None:
                browser_kwargs["user_agent"] = stealth_factory.random_profile().user_agent
        except Exception:
            pass

        try:
            if proxy_manager is not None:
                proxy_def = None
                if hasattr(proxy_manager, "next_proxy"):
                    proxy_def = proxy_manager.next_proxy()
                elif hasattr(proxy_manager, "get_proxy"):
                    proxy_def = proxy_manager.get_proxy()
                if proxy_def is not None and getattr(proxy_def, "url", None):
                    browser_kwargs["proxy"] = proxy_def.url
        except Exception:
            pass

        try:
            browser_cfg = BrowserConfig(**browser_kwargs)
        except Exception:
            try:
                browser_cfg = BrowserConfig(headless=True)
                for k, v in browser_kwargs.items():
                    try:
                        setattr(browser_cfg, k, v)
                    except Exception:
                        pass
            except Exception:
                browser_cfg = None
    except Exception:
        browser_cfg = None

    if browser_cfg is None:
        raise HTTPException(status_code=500, detail="unable to construct browser configuration")

    # Run crawler and apply per-URL enhancements
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        for url in req.urls:
            # Robots
            if robots_checker is not None:
                try:
                    if not robots_checker.is_allowed(url):
                        results.append({"url": url, "error": "disallowed_by_robots", "success": False})
                        continue
                except Exception:
                    pass

            # Rate limiter
            if rate_limiter is not None:
                try:
                    domain = urlparse(url).netloc
                    rate_limiter.acquire(domain)
                except Exception:
                    pass

            try:
                # Try to run with an explicit run config where supported
                try:
                    run_conf = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
                    res = await crawler.arun(url, config=run_conf)
                except Exception:
                    res = await crawler.arun(url)

                url_val = getattr(res, "url", url)
                html = getattr(res, "html", None) or ""
                markdown = getattr(res, "markdown", None)

                # Modal handling
                modal_result = None
                if modal_handler and html:
                    try:
                        modal_result = modal_handler.process(html)
                        html = getattr(modal_result, "cleaned_html", html)
                    except Exception:
                        modal_result = None

                # Paywall detection
                paywall_result = None
                skip_ingest = False
                if paywall_detector:
                    try:
                        paywall_result = await paywall_detector.analyze(url=url_val, html=html, text=markdown)
                        skip_ingest = bool(getattr(paywall_result, "should_skip", False))
                    except Exception:
                        paywall_result = None

                # Persist paywall
                if skip_ingest and persist_paywalls and record_paywall_detection:
                    try:
                        domain = urlparse(url_val).netloc
                        record_paywall_detection(source_id=None, domain=domain, skip_count=1, threshold=paywall_threshold, paywall_type="hard")
                    except Exception:
                        pass

                entry: Dict[str, Any] = {
                    "url": url_val,
                    "title": getattr(res, "title", None),
                    "html": html,
                    "markdown": markdown,
                    "links": getattr(res, "links", []),
                    "status_code": getattr(res, "status_code", None),
                    "success": getattr(res, "success", True),
                    "skip_ingest": skip_ingest,
                }

                if modal_result is not None:
                    entry["modal"] = {
                        "modals_detected": getattr(modal_result, "modals_detected", False),
                        "applied_cookies": getattr(modal_result, "applied_cookies", {}),
                        "notes": getattr(modal_result, "notes", []),
                    }

                if paywall_result is not None:
                    entry["paywall"] = {
                        "is_paywall": getattr(paywall_result, "is_paywall", False),
                        "confidence": float(getattr(paywall_result, "confidence", 0.0)),
                        "reasons": getattr(paywall_result, "reasons", []),
                        "metadata": getattr(paywall_result, "metadata", {}),
                    }

                results.append(entry)
            except Exception as exc:
                results.append({"url": url, "error": str(exc), "success": False})

    return {"results": results}
"""FastAPI wrapper around Crawl4AI with best-effort crawler enhancement wiring.

This module provides a small HTTP API for local crawling. It attempts to
apply the repository's crawler enhancements (modal removal, paywall
detection, UA/proxy rotation, stealth profiles, robots & rate limiting)
using best-effort imports so it remains functional even when optional
dependencies or credentials are missing.

The implementation aims for safety and minimal side effects. Production
deployments should tune environment variables (see `global.env`).
"""
from __future__ import annotations

import os
from typing import Any, Dict, List
from urllib.parse import urlparse

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="JustNewsAgent Crawl4AI Bridge")


class CrawlRequest(BaseModel):
    urls: List[str]
    mode: str = "standard"
    use_llm: bool = True


@app.get("/health")
async def health() -> Dict[str, Any]:
    return {"status": "ok", "service": "crawl4ai-bridge"}


@app.post("/crawl")
async def crawl(req: CrawlRequest) -> Dict[str, Any]:
    # Import heavy dependencies lazily to keep module import cheap
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    except Exception as exc:  # pragma: no cover - dependency may be missing in tests
        raise HTTPException(status_code=503, detail=f"crawl4ai not available: {exc}")

    # Optional enhancement helpers (best-effort)
    try:
        from agents.crawler.enhancements.modal_handler import ModalHandler
    except Exception:
        ModalHandler = None
    try:
        from agents.crawler.enhancements.paywall_detector import PaywallDetector
    except Exception:
        PaywallDetector = None
    try:
        from agents.crawler.crawler_utils import record_paywall_detection, RobotsChecker, RateLimiter
    except Exception:
        record_paywall_detection = None
        RobotsChecker = None
        RateLimiter = None
    try:
        from agents.crawler.enhancements.stealth_browser import StealthBrowserFactory
    except Exception:
        StealthBrowserFactory = None
    try:
        from agents.crawler.enhancements.ua_rotation import UserAgentProvider, UserAgentConfig
    except Exception:
        UserAgentProvider = None
        UserAgentConfig = None
    try:
        from agents.crawler.enhancements.proxy_manager import ProxyManager, PIASocks5Manager, ProxyDefinition
    except Exception:
        ProxyManager = None
        PIASocks5Manager = None
        ProxyDefinition = None

    # Runtime flags from environment
    persist_paywalls = os.getenv("CRAWL4AI_PERSIST_PAYWALLS", "true").lower() in ("1", "true", "yes")
    paywall_threshold = int(os.getenv("UNIFIED_CRAWLER_PAYWALL_SKIP_THRESHOLD", "3"))

    # Instantiate helpers (best-effort)
    modal_handler = ModalHandler() if ModalHandler is not None else None
    paywall_detector = PaywallDetector() if PaywallDetector is not None else None

    robots_checker = RobotsChecker() if RobotsChecker is not None else None
    rate_limiter = RateLimiter() if RateLimiter is not None else None

    stealth_factory = None
    try:
        if StealthBrowserFactory is not None:
            stealth_factory = StealthBrowserFactory()
    except Exception:
        stealth_factory = None

    ua_provider = None
    try:
        if UserAgentProvider is not None and UserAgentConfig is not None:
            ua_pool = None
            if stealth_factory is not None:
                try:
                    ua_pool = [p.user_agent for p in stealth_factory._profiles]
                except Exception:
                    ua_pool = None
            ua_config = UserAgentConfig(pool=ua_pool or [], per_domain_overrides={}, default=(ua_pool[0] if ua_pool else None))
            ua_provider = UserAgentProvider(ua_config)
    except Exception:
        ua_provider = None

    proxy_manager = None
    try:
        # Prefer PIA SOCKS5 if credentials exist, otherwise try a simple ProxyManager
        if PIASocks5Manager is not None and getattr(PIASocks5Manager, "is_available", lambda: False)():
            proxy_manager = PIASocks5Manager()
        elif ProxyManager is not None:
            # Accept proxies via env variable CRAWL4AI_PROXY_POOL (comma-separated)
            env_pool = os.getenv("CRAWL4AI_PROXY_POOL")
            if env_pool:
                proxies = [ProxyDefinition(url=p.strip()) for p in env_pool.split(",") if p.strip()]
                proxy_manager = ProxyManager(proxies)
            else:
                proxy_manager = ProxyManager()
    except Exception:
        proxy_manager = None

    results: List[Dict[str, Any]] = []

    # Build BrowserConfig with best-effort user_agent/proxy kwargs. We do not assume
    # BrowserConfig supports specific kwargs; use setattr guarded by try/except.
    browser_cfg = None
    try:
        # Start with a minimal kwargs dict
        browser_kwargs: dict = {"headless": True}
        # choose UA
        try:
            if ua_provider is not None:
                browser_kwargs["user_agent"] = ua_provider.choose(domain=None)
            elif stealth_factory is not None:
                browser_kwargs["user_agent"] = stealth_factory.random_profile().user_agent
        except Exception:
            # ignore UA selection errors
            pass

        # choose proxy
        try:
            if proxy_manager is not None:
                proxy_def = None
                if hasattr(proxy_manager, "next_proxy"):
                    proxy_def = proxy_manager.next_proxy()
                elif hasattr(proxy_manager, "get_proxy"):
                    proxy_def = proxy_manager.get_proxy()
                if proxy_def is not None and getattr(proxy_def, "url", None):
                    browser_kwargs["proxy"] = proxy_def.url
        except Exception:
            pass

        # Try constructing BrowserConfig with kwargs; if it fails fall back.
        try:
            browser_cfg = BrowserConfig(**browser_kwargs)
        except Exception:
            # As an additional fallback, try setting attributes after construction
            try:
                browser_cfg = BrowserConfig(headless=True)
                for k, v in browser_kwargs.items():
                    try:
                        setattr(browser_cfg, k, v)
                    except Exception:
                        # attribute not supported by this BrowserConfig implementation
                        pass
            except Exception:
                browser_cfg = None
    except Exception:
        browser_cfg = None

    if browser_cfg is None:
        raise HTTPException(status_code=500, detail="unable to construct browser configuration")

    # Run the crawler and apply enhancements per-URL
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        for url in req.urls:
            # Robots check
            if robots_checker is not None:
                try:
                    if not robots_checker.is_allowed(url):
                        results.append({"url": url, "error": "disallowed_by_robots", "success": False})
                        continue
                except Exception:
                    # If robots check fails, proceed conservatively
                    pass

            # Rate limiting per-domain
            if rate_limiter is not None:
                try:
                    domain = urlparse(url).netloc
                    rate_limiter.acquire(domain)
                except Exception:
                    pass

            try:
                # Run the crawler for this URL
                try:
                    run_conf = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
                    res = await crawler.arun(url, config=run_conf)
                except Exception:
                    # fallback to simpler call if run config not supported
                    res = await crawler.arun(url)

                url_val = getattr(res, "url", url)
                html = getattr(res, "html", None) or ""
                markdown = getattr(res, "markdown", None)

                # Modal handling (remove overlays / generate synthetic cookies)
                modal_result = None
                if modal_handler and html:
                    try:
                        modal_result = modal_handler.process(html)
                        html = getattr(modal_result, "cleaned_html", html)
                    except Exception:
                        modal_result = None

                # Paywall detection
                paywall_result = None
                skip_ingest = False
                if paywall_detector:
                    try:
                        paywall_result = await paywall_detector.analyze(url=url_val, html=html, text=markdown)
                        skip_ingest = bool(getattr(paywall_result, "should_skip", False))
                    except Exception:
                        paywall_result = None

                # Persist paywall skip if configured
                if skip_ingest and persist_paywalls and record_paywall_detection:
                    try:
                        domain = urlparse(url_val).netloc
                        # We record a single skip; the engine aggregates per-site
                        record_paywall_detection(source_id=None, domain=domain, skip_count=1, threshold=paywall_threshold, paywall_type="hard")
                    except Exception:
                        pass

                entry: Dict[str, Any] = {
                    "url": url_val,
                    "title": getattr(res, "title", None),
                    "html": html,
                    "markdown": markdown,
                    "links": getattr(res, "links", []),
                    "status_code": getattr(res, "status_code", None),
                    "success": getattr(res, "success", True),
                    "skip_ingest": skip_ingest,
                }

                if modal_result is not None:
                    entry["modal"] = {
                        "modals_detected": getattr(modal_result, "modals_detected", False),
                        "applied_cookies": getattr(modal_result, "applied_cookies", {}),
                        "notes": getattr(modal_result, "notes", []),
                    }

                if paywall_result is not None:
                    entry["paywall"] = {
                        "is_paywall": getattr(paywall_result, "is_paywall", False),
                        "confidence": float(getattr(paywall_result, "confidence", 0.0)),
                        "reasons": getattr(paywall_result, "reasons", []),
                        "metadata": getattr(paywall_result, "metadata", {}),
                    }

                results.append(entry)
            except Exception as exc:
                results.append({"url": url, "error": str(exc), "success": False})

    return {"results": results}

"""Simple FastAPI server wrapper around Crawl4AI to run as a locally managed service.

This server exposes a /health and /crawl endpoint. It's intentionally minimal so
it can be managed by systemd and fronted by the project's existing tooling.

The /crawl endpoint attempts to apply local crawler enhancements (modal
processing, paywall detection) and will optionally persist paywall skips to the
MariaDB via the existing `record_paywall_detection` helper.
"""
from __future__ import annotations

import os
from typing import Any, Dict, List
from urllib.parse import urlparse

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="JustNewsAgent Crawl4AI Bridge")


class CrawlRequest(BaseModel):
    urls: List[str]
    mode: str = "standard"
    use_llm: bool = True


@app.get("/health")
"""Simple FastAPI server wrapper around Crawl4AI to run as a locally managed service.

This server exposes a /health and /crawl endpoint. It's intentionally minimal so
it can be managed by systemd and fronted by the project's existing tooling.

The /crawl endpoint attempts to apply local crawler enhancements (modal
processing, paywall detection) and will optionally persist paywall skips to the
MariaDB via the existing `record_paywall_detection` helper.
"""
from __future__ import annotations

import os
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="JustNewsAgent Crawl4AI Bridge")


class CrawlRequest(BaseModel):
    urls: List[str]
    mode: str = "standard"
    use_llm: bool = True


@app.get("/health")
async def health() -> Dict[str, Any]:
    return {"status": "ok", "service": "crawl4ai-bridge"}


@app.post("/crawl")
async def crawl(req: CrawlRequest) -> Dict[str, Any]:
    # Import heavy dependencies here so the module import stays lightweight
    try:
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    except Exception as e:  # pragma: no cover - dependency may be missing in tests
        raise HTTPException(status_code=503, detail=f"crawl4ai not available: {e}")

    # Enhancement helpers (best-effort)
    try:
        from agents.crawler.enhancements.modal_handler import ModalHandler
    except Exception:
        ModalHandler = None  # type: ignore
    try:
        from agents.crawler.enhancements.paywall_detector import PaywallDetector
    except Exception:
        PaywallDetector = None  # type: ignore
    try:
        from agents.crawler.crawler_utils import record_paywall_detection
    except Exception:
        record_paywall_detection = None  # type: ignore

    # Anti-fingerprinting / UA / proxy helpers (best-effort)
    try:
        from agents.crawler.enhancements.ua_rotation import UserAgentProvider, UserAgentConfig
    except Exception:
        UserAgentProvider = None  # type: ignore
        UserAgentConfig = None  # type: ignore
    try:
        from agents.crawler.enhancements.stealth_browser import StealthBrowserFactory
    except Exception:
        StealthBrowserFactory = None  # type: ignore
    try:
        from agents.crawler.enhancements.proxy_manager import ProxyManager, PIASocks5Manager
    except Exception:
        ProxyManager = None  # type: ignore
        PIASocks5Manager = None  # type: ignore

    # Instantiate providers as best-effort using lightweight defaults
    UA_provider: Optional[Any] = None
    Stealth_factory: Optional[Any] = None
    Proxy_manager: Optional[Any] = None
    try:
        if StealthBrowserFactory is not None:
            Stealth_factory = StealthBrowserFactory()
    except Exception:
        Stealth_factory = None

    try:
        # Prefer PIA SOCKS5 manager if credentials available, otherwise create a no-op ProxyManager
        if PIASocks5Manager is not None and getattr(PIASocks5Manager, "is_available", lambda: False)():
            Proxy_manager = PIASocks5Manager()
        elif ProxyManager is not None:
            Proxy_manager = ProxyManager()
    except Exception:
        Proxy_manager = None

    try:
        if UserAgentProvider is not None and Stealth_factory is not None:
            # Use a single stealth-derived UA as a minimal pool
            profile = Stealth_factory.random_profile()
            ua_config = UserAgentConfig(pool=[profile.user_agent], per_domain_overrides={}, default=profile.user_agent)
            UA_provider = UserAgentProvider(config=ua_config)
    except Exception:
        UA_provider = None

    persist_paywalls = os.getenv("CRAWL4AI_PERSIST_PAYWALLS", "true").lower() in ("1", "true", "yes")
    paywall_threshold = int(os.getenv("UNIFIED_CRAWLER_PAYWALL_SKIP_THRESHOLD", "3"))

    modal_handler = ModalHandler() if ModalHandler is not None else None
    paywall_detector = PaywallDetector() if PaywallDetector is not None else None

    results: List[Dict[str, Any]] = []

    # Build a BrowserConfig with optional UA / proxy if the classes accept those kwargs;
    # fall back to a simple headless config on any failure.
    try:
        browser_kwargs: dict = {"headless": True}
        if UA_provider is not None:
            try:
                browser_kwargs["user_agent"] = UA_provider.choose(domain=None)
            except Exception:
                pass
        elif Stealth_factory is not None:
            try:
                browser_kwargs["user_agent"] = Stealth_factory.random_profile().user_agent
            except Exception:
        # Enhancement helpers (best-effort)
        try:
            from agents.crawler.enhancements.modal_handler import ModalHandler
        except Exception:
            ModalHandler = None
        try:
            from agents.crawler.enhancements.paywall_detector import PaywallDetector
        except Exception:
            PaywallDetector = None
        try:
            from agents.crawler.crawler_utils import record_paywall_detection, RobotsChecker, RateLimiter
        except Exception:
            record_paywall_detection = None
            RobotsChecker = None
            RateLimiter = None
        try:
            from agents.crawler.enhancements.stealth_browser import StealthBrowserFactory
        except Exception:
            StealthBrowserFactory = None
        try:
            from agents.crawler.enhancements.ua_rotation import UserAgentProvider, UserAgentConfig
        except Exception:
            UserAgentProvider = None
            UserAgentConfig = None
        try:
            from agents.crawler.enhancements.proxy_manager import ProxyManager, PIASocks5Manager, ProxyDefinition
        except Exception:
            ProxyManager = None
            PIASocks5Manager = None
            ProxyDefinition = None
        try:
            browser_cfg = BrowserConfig(headless=True)
        except Exception:
            browser_cfg = None

    # Ensure we have a BrowserConfig to open the crawler with; if not possible raise
    if browser_cfg is None:
        raise HTTPException(status_code=500, detail="unable to construct browser configuration")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        for url in req.urls:
            try:
                res = await crawler.arun(url, config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS))
                url_val = getattr(res, "url", url)
                html = getattr(res, "html", None) or ""
                markdown = getattr(res, "markdown", None)

                # Modal handling
                modal_result = None
                if modal_handler and html:
                    try:
                        modal_result = modal_handler.process(html)
                        html = getattr(modal_result, "cleaned_html", html)
                    except Exception:
                        modal_result = None

                # Paywall detection
                paywall_result = None
                skip_ingest = False
                if paywall_detector:
                    try:
                        paywall_result = await paywall_detector.analyze(url=url_val, html=html, text=markdown)
                        skip_ingest = bool(getattr(paywall_result, "should_skip", False))
                    except Exception:
                        paywall_result = None

                # Persist paywall if configured and detector flagged skip
                if skip_ingest and persist_paywalls and record_paywall_detection:
                    try:
                        domain = urlparse(url_val).netloc
                        # Record a single skip; the central engine aggregates per-site skips
                        record_paywall_detection(source_id=None, domain=domain, skip_count=1, threshold=paywall_threshold, paywall_type="hard")
                    except Exception:
                        pass

                entry: Dict[str, Any] = {
                    "url": url_val,
                    "title": getattr(res, "title", None),
                    "html": html,
                    "markdown": markdown,
                    "links": getattr(res, "links", []),
                    "status_code": getattr(res, "status_code", None),
                    "success": getattr(res, "success", True),
                    "skip_ingest": skip_ingest,
                }

                if modal_result is not None:
                    entry["modal"] = {
                        "modals_detected": getattr(modal_result, "modals_detected", False),
                        "applied_cookies": getattr(modal_result, "applied_cookies", {}),
                        "notes": getattr(modal_result, "notes", []),
                    }

                if paywall_result is not None:
                    entry["paywall"] = {
                        "is_paywall": getattr(paywall_result, "is_paywall", False),
                        "confidence": float(getattr(paywall_result, "confidence", 0.0)),
                        "reasons": getattr(paywall_result, "reasons", []),
                        "metadata": getattr(paywall_result, "metadata", {}),
                    }

                results.append(entry)
            except Exception as e:
                results.append({"url": url, "error": str(e), "success": False})

    return {"results": results}
        UserAgentConfig = None
    try:
        from agents.crawler.enhancements.stealth_browser import StealthBrowserFactory
    except Exception:
        StealthBrowserFactory = None
    try:
        from agents.crawler.enhancements.proxy_manager import ProxyManager, PIASocks5Manager
    except Exception:
        ProxyManager = None
        PIASocks5Manager = None

    # Instantiate providers as best-effort using defaults (no external config required)
    try:
        if StealthBrowserFactory is not None:
            Stealth_factory = StealthBrowserFactory()
    except Exception:
        Stealth_factory = None
    try:
        # Try PIA SOCKS5 if available (reads env creds); otherwise create an empty ProxyManager
        if PIASocks5Manager is not None and getattr(PIASocks5Manager, "is_available", lambda: False)():
            Proxy_manager = PIASocks5Manager()
        elif ProxyManager is not None:
            Proxy_manager = ProxyManager()
    except Exception:
        Proxy_manager = None

    try:
        if UserAgentProvider is not None and Stealth_factory is not None:
            # Build a minimal config using stealth profiles as UA candidates
            profiles = [{"user_agent": p.user_agent} for p in [Stealth_factory.random_profile()]]
            ua_config = UserAgentConfig(pool=[p["user_agent"] for p in profiles], per_domain_overrides={}, default=profiles[0]["user_agent"])
            UA_provider = UserAgentProvider(config=ua_config)
    except Exception:
        UA_provider = None

    persist_paywalls = os.getenv("CRAWL4AI_PERSIST_PAYWALLS", "true").lower() in ("1", "true", "yes")
    paywall_threshold = int(os.getenv("UNIFIED_CRAWLER_PAYWALL_SKIP_THRESHOLD", "3"))

    modal_handler = ModalHandler() if ModalHandler is not None else None
    paywall_detector = PaywallDetector() if PaywallDetector is not None else None

    results = []
    # Attempt to construct a BrowserConfig with UA/proxy if available; fall back safely
    browser_cfg = None
    try:
        browser_kwargs = {"headless": True}
        # Choose a default UA from providers if present
        try:
            if UA_provider is not None:
                ua_choice = UA_provider.choose(domain=None)
                if ua_choice:
                    browser_kwargs["user_agent"] = ua_choice
            elif Stealth_factory is not None:
                browser_kwargs["user_agent"] = Stealth_factory.random_profile().user_agent
        except Exception:
            pass

        # Choose a proxy if available
        try:
            if Proxy_manager is not None:
                proxy_def = None
                # PIASocks5Manager uses get_proxy/get_proxy_url
                if hasattr(Proxy_manager, "next_proxy"):
                    proxy_def = Proxy_manager.next_proxy()
                elif hasattr(Proxy_manager, "get_proxy"):
                    proxy_def = Proxy_manager.get_proxy()
                if proxy_def is not None:
                    browser_kwargs["proxy"] = proxy_def.url
        except Exception:
            pass

        browser_cfg = BrowserConfig(**browser_kwargs)
    except Exception:
        # BrowserConfig didn't accept our extras; fallback to simple config
        try:
            browser_cfg = BrowserConfig(headless=True)
        except Exception:
            browser_cfg = None

    async with AsyncWebCrawler(config=browser_cfg or BrowserConfig(headless=True)) as crawler:
                        {
                            "modal": {
                                "modals_detected": modal_result.modals_detected,
                                "applied_cookies": modal_result.applied_cookies,
                                "notes": modal_result.notes,
                            }
                        }
                    )

                if paywall_result is not None:
                    entry.update(
                        {
                            "paywall": {
                                "is_paywall": paywall_result.is_paywall,
                                "confidence": float(paywall_result.confidence),
                                "reasons": paywall_result.reasons,
                                "metadata": paywall_result.metadata,
                            }
                        }
                    )

                results.append(entry)
            except Exception as e:
                results.append({"url": url, "error": str(e), "success": False})

    return {"results": results}
