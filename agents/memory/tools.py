"""
Memory Agent Tools - Utility Functions
=====================================

Core utilities for the memory agent:
- Embedding model management
- Article storage operations
- Vector search functionality
- Feedback logging
- Training data collection

Architecture:
- Shared embedding model with caching
- Database connection pooling
- GPU acceleration support
- Comprehensive error handling
"""

import json
import os
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional
from time import perf_counter

import numpy as np
import requests

from common.observability import get_logger
from common.stage_b_metrics import get_stage_b_metrics

try:
    import torch
except Exception:
    torch = None

# Import database utilities
from agents.common.database import execute_query, execute_query_single
from agents.common.database import get_db_connection as get_pooled_connection
from common.url_normalization import hash_article_url, normalize_article_url

# Configure centralized logging
logger = get_logger(__name__)

# Environment variables
FEEDBACK_LOG = os.environ.get("MEMORY_FEEDBACK_LOG", "./feedback_memory.log")
EMBEDDING_MODEL_NAME = os.environ.get("SENTENCE_TRANSFORMER_MODEL", "all-MiniLM-L6-v2")
MEMORY_AGENT_PORT = int(os.environ.get("MEMORY_AGENT_PORT", 8007))
POSTGRES_HOST = os.environ.get("POSTGRES_HOST")
POSTGRES_DB = os.environ.get("POSTGRES_DB")
POSTGRES_USER = os.environ.get("POSTGRES_USER")
POSTGRES_PASSWORD = os.environ.get("POSTGRES_PASSWORD")

# Canonical cache folder for shared embedding model
DEFAULT_MODEL_CACHE = os.environ.get("MEMORY_MODEL_CACHE") or str(Path('./agents/memory/models').resolve())


def log_feedback(event: str, details: dict):
    """Logs feedback to a file."""
    try:
        with open(FEEDBACK_LOG, "a", encoding="utf-8") as f:
            f.write(f"{datetime.now(timezone.utc).isoformat()}\t{event}\t{details}\n")
    except Exception as e:
        logger.error(f"Error logging feedback: {e}")


def get_embedding_model():
    """Return a SentenceTransformer instance, using the shared helper when available."""
    try:
        from agents.common.embedding import get_shared_embedding_model
        # Use a canonical cache folder and device so cached instances are reused
        device = None
        if torch is not None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        return get_shared_embedding_model(EMBEDDING_MODEL_NAME, cache_folder=DEFAULT_MODEL_CACHE, device=device)
    except Exception:
        # Fallback: use agent-local models directory
        try:
            from agents.common.embedding import get_shared_embedding_model
            agent_cache = os.environ.get('MEMORY_MODEL_CACHE') or str(Path('./agents/memory/models').resolve())
            device = None
            if torch is not None:
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            return get_shared_embedding_model(EMBEDDING_MODEL_NAME, cache_folder=agent_cache, device=device)
        except Exception as e:
            logger.warning(f"Could not load shared embedding model: {e}")
            # Return None - caller should handle this gracefully
            return None


def _parse_publication_date(value: Any) -> Optional[datetime]:
    if not value:
        return None
    if isinstance(value, datetime):
        return value
    if isinstance(value, str):
        candidate = value.strip()
        if not candidate:
            return None
        try:
            if candidate.endswith("Z"):
                candidate = candidate[:-1] + "+00:00"
            return datetime.fromisoformat(candidate)
        except ValueError:
            return None
    return None


def save_article(content: str, metadata: dict, embedding_model=None) -> dict:
    """Saves an article to the database and generates an embedding for the content.

    Args:
        content: Article text to embed and store.
        metadata: Arbitrary metadata dict to store alongside the article.
        embedding_model: Optional pre-initialized SentenceTransformer instance.
            If not provided, a new model will be created via get_embedding_model().
    """
    metrics = get_stage_b_metrics()
    try:
        metadata = metadata or {}

        raw_url = metadata.get("url")
        canonical_url = metadata.get("canonical") or raw_url
        normalized_url = metadata.get("normalized_url") or normalize_article_url(raw_url or "", canonical_url)
        normalized_url = normalized_url or None
        hash_algorithm = (metadata.get("url_hash_algorithm") or os.environ.get("ARTICLE_URL_HASH_ALGO", "sha256")).lower()
        hash_candidate = metadata.get("url_hash") or hash_article_url(
            normalized_url or canonical_url or raw_url or "",
            algorithm=hash_algorithm,
        )

        if not hash_candidate and normalized_url:
            hash_candidate = hash_article_url(normalized_url, algorithm=hash_algorithm)
        hash_value = hash_candidate or None

        duplicate_lookup_id = None
        if hash_value:
            duplicate = execute_query_single(
                "SELECT id FROM articles WHERE url_hash = %s",
                (hash_value,)
            )
            if duplicate:
                duplicate_lookup_id = duplicate.get("id")
        elif normalized_url:
            duplicate = execute_query_single(
                "SELECT id FROM articles WHERE normalized_url = %s",
                (normalized_url,)
            )
            if duplicate:
                duplicate_lookup_id = duplicate.get("id")

        if duplicate_lookup_id is not None:
            logger.info(
                "Article with hash %s already exists (ID: %s), skipping duplicate",
                hash_value,
                duplicate_lookup_id,
            )
            metrics.record_ingestion("duplicate")
            return {
                "status": "duplicate",
                "article_id": duplicate_lookup_id,
                "message": "Article already exists",
            }

        # Use provided model if available to avoid re-loading model per-call
        cache_label = "provided" if embedding_model is not None else "shared"
        if embedding_model is None:
            embedding_model = get_embedding_model()
            if embedding_model is None:
                metrics.record_embedding("model_unavailable")
                logger.error("No embedding model available for article storage")
                metrics.record_ingestion("embedding_model_unavailable")
                return {"error": "embedding_model_unavailable"}
            cache_label = "shared"

        # encode may return numpy array; convert later to list of floats
        encode_start = perf_counter()
        try:
            embedding = embedding_model.encode(content)
            encode_duration = perf_counter() - encode_start
            metrics.observe_embedding_latency(cache_label, encode_duration)
            metrics.record_embedding("success")
        except Exception as encoding_error:
            encode_duration = perf_counter() - encode_start
            metrics.observe_embedding_latency(cache_label, encode_duration)
            metrics.record_embedding("error")
            logger.error("Embedding generation failed: %s", encoding_error)
            metrics.record_ingestion("error")
            return {"error": "embedding_generation_failed"}

        try:
            metadata_payload = json.dumps(metadata)
        except Exception:
            metadata_payload = json.dumps({"raw": str(metadata)})

        authors: List[str] = metadata.get("authors") or []
        if isinstance(authors, str):
            authors = [authors]
        tags: List[str] = metadata.get("tags") or []
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(",") if t.strip()]

        publication_dt = _parse_publication_date(metadata.get("publication_date"))
        collection_dt = _parse_publication_date(metadata.get("collection_timestamp"))
        if collection_dt is None:
            collection_dt = datetime.now(timezone.utc)

        review_reasons_json = json.dumps(metadata.get("review_reasons") or [])

        insertion_params = (
            raw_url,
            metadata.get("title"),
            content,
            metadata.get("summary"),
            bool(metadata.get("analyzed", False)),
            metadata.get("source_id"),
            normalized_url,
            hash_value,
            hash_algorithm,
            metadata.get("language"),
            metadata.get("section"),
            tags or None,
            json.dumps(authors) if authors else None,
            metadata.get("raw_html_ref"),
            float(metadata.get("confidence", 0.0)) if metadata.get("confidence") is not None else None,
            bool(metadata.get("needs_review", False)),
            review_reasons_json,
            json.dumps(metadata.get("extraction_metadata") or {}),
            json.dumps(metadata.get("structured_metadata") or {}),
            publication_dt,
            metadata_payload,
            collection_dt,
            list(map(float, embedding)),
        )

        inserted = execute_query_single(
            """
            INSERT INTO articles (
                url,
                title,
                content,
                summary,
                analyzed,
                source_id,
                normalized_url,
                url_hash,
                url_hash_algo,
                language,
                section,
                tags,
                authors,
                raw_html_ref,
                extraction_confidence,
                needs_review,
                review_reasons,
                extraction_metadata,
                structured_metadata,
                publication_date,
                metadata,
                collection_timestamp,
                embedding,
                created_at,
                updated_at
            )
            VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s::jsonb, %s, %s, %s, %s::jsonb, %s::jsonb, %s::jsonb, %s, %s::jsonb, %s, %s, NOW(), NOW()
            )
            RETURNING id
            """,
            insertion_params,
        )

        next_id = inserted["id"] if isinstance(inserted, dict) else inserted

        log_feedback("save_article", {"status": "success", "article_id": next_id})

        result = {"status": "success", "article_id": next_id, "id": next_id}

        # Collect prediction for training
        try:
            from training_system import collect_prediction
            collect_prediction(
                agent_name="memory",
                task_type="article_storage",
                input_text=content,
                prediction=result,
                confidence=0.95,  # High confidence for successful storage
                source_url=""
            )
            logger.debug("ðŸ“Š Training data collected for article storage")
        except ImportError:
            logger.debug("Training system not available - skipping data collection")
        except Exception as e:
            logger.warning(f"Failed to collect training data: {e}")

        # Return both 'article_id' and legacy 'id' key for backward compatibility
        metrics.record_ingestion("success")
        return result
    except Exception as e:
        logger.error(f"Error saving article: {e}")
        metrics.record_ingestion("error")
        return {"error": str(e)}


def vector_search_articles(query: str, top_k: int = 5) -> list:
    """Performs a vector search for articles using the memory agent."""
    url = f"http://localhost:{MEMORY_AGENT_PORT}/vector_search_articles"
    try:
        response = requests.post(url, json={"query": query, "top_k": top_k}, timeout=5)
        response.raise_for_status()
        res = response.json()
        # Coerce a few common shapes from test fakes: allow list or dict with 'results'
        if isinstance(res, list):
            return res
        if isinstance(res, dict):
            # prefer explicit results key
            if 'results' in res and isinstance(res['results'], list):
                return res['results']
            # sometimes test fakes return empty dict
            return []
        return []
    except requests.exceptions.RequestException as e:
        logger.warning(f"vector_search_articles: memory agent request failed: {e}")
        return []


def vector_search_articles_local(query: str, top_k: int = 5, embedding_model=None) -> list:
    """Local in-process vector search implementation.

    This avoids making an HTTP call to the same process when the endpoint is
    executed inside the memory agent. It queries the articles table for stored
    embeddings and returns the top_k nearest articles by cosine similarity.
    """
    try:
        # Retrieve id, content, metadata and embedding from the DB using new connection pooling
        rows = execute_query("SELECT id, content, metadata, embedding FROM articles WHERE embedding IS NOT NULL")
        if not rows:
            return []
    except Exception as e:
        logger.warning(f"vector_search_articles_local: DB query failed: {e}")
        return []

    # Build embeddings matrix and compute cosine similarities
    try:
        # Use provided model or get one
        if embedding_model is None:
            embedding_model = get_embedding_model()

        if embedding_model is None:
            logger.error("No embedding model available for vector search")
            return []

        # Load stored embeddings and ids
        ids = []
        contents = {}
        metas = {}
        embeddings = []
        for r in rows:
            ids.append(r['id'])
            contents[r['id']] = r['content']
            metas[r['id']] = r.get('metadata')
            emb = r.get('embedding')
            if emb is None:
                emb = []
            embeddings.append(np.array(emb, dtype=float))

        if len(embeddings) == 0:
            return []

        # Compute query embedding
        q_emb = embedding_model.encode(query)
        q_emb = np.array(q_emb, dtype=float)

        M = np.vstack(embeddings)
        # Normalize
        def _norm(a):
            n = np.linalg.norm(a)
            return a / n if n != 0 else a

        Mn = np.apply_along_axis(_norm, 1, M)
        qn = _norm(q_emb)
        sims = Mn.dot(qn)
        # Get top_k indices
        top_idx = np.argsort(-sims)[:top_k]
        results = []
        for i in top_idx:
            aid = ids[int(i)]
            results.append({
                "id": int(aid),
                "score": float(sims[int(i)]),
                "content": contents[aid],
                "metadata": metas.get(aid),
            })

        # Collect prediction for training
        try:
            from training_system import collect_prediction
            confidence = min(0.9, max(0.1, float(np.mean(sims[top_idx])))) if len(top_idx) > 0 else 0.5
            collect_prediction(
                agent_name="memory",
                task_type="vector_search",
                input_text=query,
                prediction={"results": results, "top_k": top_k},
                confidence=confidence,
                source_url=""
            )
            logger.debug(f"ðŸ“Š Training data collected for vector search (confidence: {confidence:.3f})")
        except ImportError:
            logger.debug("Training system not available - skipping data collection")
        except Exception as e:
            logger.warning(f"Failed to collect training data: {e}")

        return results
    except Exception:
        logger.exception("vector_search_articles_local: error computing similarities")
        return []