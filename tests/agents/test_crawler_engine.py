"""
Comprehensive tests for CrawlerEngine class

Tests cover initialization, strategy determination, all crawling modes,
multi-site concurrent crawling, article ingestion, AI analysis delegation,
performance monitoring, and error handling.
"""

import json
from unittest.mock import Mock, patch, AsyncMock

import pytest
import pytest_asyncio

from agents.crawler.crawler_engine import CrawlerEngine
from agents.crawler.main import execute_crawl
from agents.sites.generic_site_crawler import SiteConfig


class TestCrawlerEngine:
    """Test suite for CrawlerEngine class"""

    @pytest.fixture
    def mock_site_config(self):
        """Create a mock SiteConfig for testing"""
        return SiteConfig({
            'id': 1,
            'name': 'Test Site',
            'domain': 'testsite.com',
            'url': 'https://testsite.com',
            'crawling_strategy': 'generic'
        })

    @pytest.fixture
    def mock_bbc_config(self):
        """Create a mock BBC SiteConfig for ultra-fast testing"""
        return SiteConfig({
            'id': 2,
            'name': 'BBC News',
            'domain': 'bbc.co.uk',
            'url': 'https://bbc.co.uk/news',
            'crawling_strategy': 'ultra_fast'
        })

    @pytest.fixture
    def mock_complex_config(self):
        """Create a mock complex site config for AI-enhanced testing"""
        return SiteConfig({
            'id': 3,
            'name': 'New York Times',
            'domain': 'nytimes.com',
            'url': 'https://nytimes.com',
            'crawling_strategy': 'ai_enhanced'
        })

    @pytest_asyncio.fixture
    async def crawler_engine(self):
        """Create a CrawlerEngine instance for testing"""
        engine = CrawlerEngine()
        yield engine
        # Cleanup after test
        await engine._cleanup()

    @pytest.mark.asyncio
    async def test_initialization(self):
        """Test CrawlerEngine initialization and component setup"""
        with patch('agents.crawler.crawler_engine.initialize_connection_pool'), \
             patch('agents.crawler.crawler_engine.create_crawling_performance_table'), \
             patch('agents.crawler.crawler_engine.start_performance_monitoring'):

            engine = CrawlerEngine()

            # Test core components are initialized
            assert engine.rate_limiter is not None
            assert engine.robots_checker is not None
            assert engine.multi_site_crawler is not None
            assert engine.performance_monitor is not None
            assert engine.performance_optimizer is not None

            # Test performance metrics structure
            expected_metrics = {
                "start_time": engine.performance_metrics["start_time"],
                "articles_processed": 0,
                "sites_crawled": 0,
                "errors": 0,
                "mode_usage": {
                    "ultra_fast": 0,
                    "ai_enhanced": 0,
                    "generic": 0,
                    "crawl4ai_profiled": 0,
                },
            }
            assert engine.performance_metrics == expected_metrics

            # Test caches are initialized
            assert isinstance(engine.strategy_cache, dict)
            assert isinstance(engine.performance_history, dict)

            await engine._cleanup()

    @pytest.mark.asyncio
    async def test_async_context_manager(self):
        """Test async context manager functionality"""
        with patch('agents.crawler.crawler_engine.initialize_connection_pool'), \
             patch('agents.crawler.crawler_engine.create_crawling_performance_table'), \
             patch('agents.crawler.crawler_engine.start_performance_monitoring'), \
             patch('agents.crawler.crawler_engine.CrawlerEngine._cleanup_orphaned_processes') as mock_cleanup:

            async with CrawlerEngine() as engine:
                assert isinstance(engine, CrawlerEngine)
                # Engine should be usable within context
                assert engine.rate_limiter is not None

            # Cleanup should be called on exit
            mock_cleanup.assert_called()

    @pytest.mark.asyncio
    async def test_determine_optimal_strategy_ultra_fast_sites(self, crawler_engine, mock_bbc_config):
        """Test strategy determination for ultra-fast sites"""
        # Test BBC domain gets ultra_fast strategy
        strategy = await crawler_engine._determine_optimal_strategy(mock_bbc_config)
        assert strategy == "ultra_fast"

    @pytest.mark.asyncio
    async def test_determine_optimal_strategy_ai_enhanced_sites(self, crawler_engine, mock_complex_config):
        """Test strategy determination for complex sites"""
        # Test NYT domain gets ai_enhanced strategy
        strategy = await crawler_engine._determine_optimal_strategy(mock_complex_config)
        assert strategy == "ai_enhanced"

    @pytest.mark.asyncio
    async def test_determine_optimal_strategy_generic_default(self, crawler_engine, mock_site_config):
        """Test strategy determination defaults to generic"""
        strategy = await crawler_engine._determine_optimal_strategy(mock_site_config)
        assert strategy == "generic"

    @pytest.mark.asyncio
    async def test_determine_optimal_strategy_performance_cache(self, crawler_engine, mock_site_config):
        """Test strategy caching functionality"""
        # Mock performance history to trigger caching
        with patch('agents.crawler.crawler_engine.get_source_performance_history', return_value=[
            {"strategy_used": "ultra_fast", "articles_per_second": 10.0},
            {"strategy_used": "generic", "articles_per_second": 2.0}
        ]):
            # First call should cache the result
            strategy1 = await crawler_engine._determine_optimal_strategy(mock_site_config)
            assert strategy1 == "ultra_fast"

            # Second call should use cache
            strategy2 = await crawler_engine._determine_optimal_strategy(mock_site_config)
            assert strategy2 == "ultra_fast"

            # Verify cache was populated
            cache_key = f"{mock_site_config.domain}_{mock_site_config.source_id}"
            assert cache_key in crawler_engine.strategy_cache
            assert crawler_engine.strategy_cache[cache_key] == "ultra_fast"

    @pytest.mark.asyncio
    async def test_crawl_ultra_fast_mode_bbc(self, crawler_engine, mock_bbc_config):
        """Test ultra-fast crawling mode for BBC"""
        mock_articles = [
            {"title": "Test Article 1", "url": "https://bbc.co.uk/article1", "content": "Content 1"},
            {"title": "Test Article 2", "url": "https://bbc.co.uk/article2", "content": "Content 2"}
        ]

        with patch('agents.sites.bbc_crawler.UltraFastBBCCrawler') as mock_crawler_class, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            from unittest.mock import AsyncMock
            mock_crawler = Mock()
            mock_crawler.run_ultra_fast_crawl = AsyncMock(return_value={"articles": mock_articles})
            mock_crawler_class.return_value = mock_crawler

            result = await crawler_engine._crawl_ultra_fast_mode(mock_bbc_config, max_articles=2)

            assert result == mock_articles
            assert crawler_engine.performance_metrics["mode_usage"]["ultra_fast"] == 1
            mock_crawler.run_ultra_fast_crawl.assert_called_once_with(2, skip_ingestion=True)

    @pytest.mark.asyncio
    async def test_crawl_ultra_fast_mode_fallback(self, crawler_engine, mock_site_config):
        """Test ultra-fast mode fallback to generic for non-BBC sites"""
        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]

        with patch('agents.crawler.crawler_engine.GenericSiteCrawler') as mock_crawler_class, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            from unittest.mock import AsyncMock
            mock_crawler = Mock()
            mock_crawler.crawl_site = AsyncMock(return_value=mock_articles)
            mock_crawler_class.return_value = mock_crawler

            result = await crawler_engine._crawl_ultra_fast_mode(mock_site_config, max_articles=1)

            assert result == mock_articles
            assert crawler_engine.performance_metrics["mode_usage"]["ultra_fast"] == 1
            mock_crawler.crawl_site.assert_called_once_with(1)

    @pytest.mark.asyncio
    async def test_crawl_ai_enhanced_mode(self, crawler_engine, mock_complex_config):
        """Test AI-enhanced crawling mode"""
        mock_articles = [
            {"title": "Test Article", "url": "https://nytimes.com/article", "content": "Content"}
        ]

        with patch.object(crawler_engine, '_crawl_generic_mode', return_value=mock_articles) as mock_generic, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            result = await crawler_engine._crawl_ai_enhanced_mode(mock_complex_config, max_articles=1)

            assert result == mock_articles
            assert crawler_engine.performance_metrics["mode_usage"]["ai_enhanced"] == 1
            mock_generic.assert_called_once_with(mock_complex_config, 1)

    @pytest.mark.asyncio
    async def test_crawl_generic_mode(self, crawler_engine, mock_site_config):
        """Test generic crawling mode"""
        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]

        with patch('agents.crawler.crawler_engine.GenericSiteCrawler') as mock_crawler_class, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            from unittest.mock import AsyncMock
            mock_crawler = Mock()
            mock_crawler.crawl_site = AsyncMock(return_value=mock_articles)
            mock_crawler_class.return_value = mock_crawler

            result = await crawler_engine._crawl_generic_mode(mock_site_config, max_articles=1)

            assert result == mock_articles
            assert crawler_engine.performance_metrics["mode_usage"]["generic"] == 1
            mock_crawler.crawl_site.assert_called_once_with(1)

    @pytest.mark.asyncio
    async def test_crawl_with_profile_crawl4ai(self, crawler_engine, mock_site_config):
        """Test crawling with Crawl4AI profile"""
        # This test is complex to mock due to relative imports inside functions
        # The profile functionality is tested in integration tests
        pass

    @pytest.mark.asyncio
    async def test_crawl_with_profile_generic_fallback(self, crawler_engine, mock_site_config):
        """Test profile crawling falls back to generic"""
        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]
        profile = {"engine": "generic"}

        with patch.object(crawler_engine, '_crawl_generic_mode', return_value=mock_articles) as mock_generic, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            result = await crawler_engine._crawl_with_profile(mock_site_config, profile, 3)

            assert result == mock_articles
            mock_generic.assert_called_once_with(mock_site_config, 3)

    @pytest.mark.asyncio
    async def test_crawl_with_profile_crawl4ai_import_error(self, crawler_engine, mock_site_config):
        """Test profile crawling handles Crawl4AI import error"""
        # This test is complex to mock due to relative imports inside functions
        # The fallback behavior is tested in test_crawl_with_profile_unknown_engine
        pass

    @pytest.mark.asyncio
    async def test_apply_ai_analysis_success(self, crawler_engine):
        """Test successful AI analysis application"""
        article = {
            "title": "Test Article",
            "content": "This is a test article with substantial content for analysis that is definitely longer than 100 characters to trigger the AI analysis functionality properly.",
            "url": "https://testsite.com/article"
        }

        with patch('agents.crawler.crawler_engine.call_analyst_tool') as mock_call:
            mock_call.side_effect = [
                0.8,  # sentiment score
                ["politics", "economy"],  # topics
            ]

            result = await crawler_engine._apply_ai_analysis(article)

            assert result["sentiment"] == {"score": 0.8}
            assert result["topics"] == ["politics", "economy"]
            assert result["ai_analysis_applied"] is True

            assert mock_call.call_count == 2
            mock_call.assert_any_call('score_sentiment', article["content"])
            mock_call.assert_any_call('extract_topics', article["content"])

    @pytest.mark.asyncio
    async def test_apply_ai_analysis_short_content(self, crawler_engine):
        """Test AI analysis skips short content"""
        article = {
            "title": "Test Article",
            "content": "Short",
            "url": "https://testsite.com/article"
        }

        with patch('agents.crawler.crawler_engine.call_analyst_tool') as mock_call:
            result = await crawler_engine._apply_ai_analysis(article)

            # Should return article unchanged
            assert result == article
            mock_call.assert_not_called()

    @pytest.mark.asyncio
    async def test_apply_ai_analysis_error_handling(self, crawler_engine):
        """Test AI analysis handles errors gracefully"""
        article = {
            "title": "Test Article",
            "content": "This is a test article with substantial content for analysis.",
            "url": "https://testsite.com/article"
        }

        with patch('agents.crawler.crawler_engine.call_analyst_tool', side_effect=Exception("Analysis failed")):
            result = await crawler_engine._apply_ai_analysis(article)

            # Should return article without AI analysis
            assert result == article
            assert "sentiment" not in result
            assert "topics" not in result
            assert result.get("ai_analysis_applied") is not True

    @pytest.mark.asyncio
    async def test_crawl_site_ultra_fast(self, crawler_engine, mock_bbc_config):
        """Test crawl_site with ultra_fast strategy"""
        mock_articles = [
            {"title": "Test Article", "url": "https://bbc.co.uk/article", "content": "Content"}
        ]

        with patch.object(crawler_engine, '_crawl_ultra_fast_mode', return_value=mock_articles) as mock_ultra_fast, \
             patch.object(crawler_engine, '_determine_optimal_strategy', return_value='ultra_fast'):

            result = await crawler_engine.crawl_site(mock_bbc_config, max_articles=1)

            assert result == mock_articles
            mock_ultra_fast.assert_called_once_with(mock_bbc_config, 1)

    @pytest.mark.asyncio
    async def test_crawl_site_ai_enhanced(self, crawler_engine, mock_complex_config):
        """Test crawl_site with ai_enhanced strategy"""
        mock_articles = [
            {"title": "Test Article", "url": "https://nytimes.com/article", "content": "Content"}
        ]

        with patch.object(crawler_engine, '_crawl_ai_enhanced_mode', return_value=mock_articles) as mock_ai_enhanced, \
             patch.object(crawler_engine, '_determine_optimal_strategy', return_value='ai_enhanced'):

            result = await crawler_engine.crawl_site(mock_complex_config, max_articles=1)

            assert result == mock_articles
            mock_ai_enhanced.assert_called_once_with(mock_complex_config, 1)

    @pytest.mark.asyncio
    async def test_crawl_site_generic(self, crawler_engine, mock_site_config):
        """Test crawl_site with generic strategy"""
        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]

        with patch.object(crawler_engine, '_crawl_generic_mode', return_value=mock_articles) as mock_generic, \
             patch.object(crawler_engine, '_determine_optimal_strategy', return_value='generic'):

            result = await crawler_engine.crawl_site(mock_site_config, max_articles=1)

            assert result == mock_articles
            mock_generic.assert_called_once_with(mock_site_config, 1)

    @pytest.mark.asyncio
    async def test_crawl_multiple_sites_basic(self, crawler_engine, mock_site_config):
        """Test basic multi-site crawling functionality"""
        site_configs = [mock_site_config]
        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]

        with patch.object(crawler_engine, 'crawl_site', return_value=mock_articles) as _mock_crawl_site, \
             patch.object(crawler_engine, '_ingest_articles') as mock_ingest, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            mock_ingest.return_value = {
                'new_articles': 1,
                'duplicates': 0,
                'errors': 0,
                'details': [{'url': 'https://testsite.com/article', 'status': 'new'}]
            }

            result = await crawler_engine.crawl_multiple_sites(site_configs, max_articles_per_site=1)

            # Verify basic structure
            assert result["unified_crawl"] is True
            assert result["sites_crawled"] == 1
            assert result["total_articles"] == 1
            assert result["articles_ingested"] == 1
            assert result["duplicates_skipped"] == 0
            assert result["ingestion_errors"] == 0
            assert "processing_time_seconds" in result
            assert "articles_per_second" in result
            assert result["site_breakdown"]["testsite.com"] == 1
            assert len(result["articles"]) == 1

            # Verify performance metrics updated
            assert crawler_engine.performance_metrics["articles_processed"] == 1
            assert crawler_engine.performance_metrics["sites_crawled"] == 1

    @pytest.mark.asyncio
    async def test_crawl_multiple_sites_concurrent(self, crawler_engine):
        """Test concurrent multi-site crawling with semaphore"""
        site_configs = [
            SiteConfig({'id': 1, 'name': 'Site 1', 'domain': 'site1.com', 'url': 'https://site1.com'}),
            SiteConfig({'id': 2, 'name': 'Site 2', 'domain': 'site2.com', 'url': 'https://site2.com'}),
        ]

        with patch.object(crawler_engine, 'crawl_site', return_value=[
            {"title": "Article 1", "url": "https://site1.com/article", "content": "Content 1"}
            ]) as mock_crawl_site, \
             patch.object(crawler_engine, '_ingest_articles') as mock_ingest, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            mock_ingest.return_value = {
                'new_articles': 1,
                'duplicates': 0,
                'errors': 0,
                'details': [{'url': 'https://site1.com/article', 'status': 'new'}]
            }

            result = await crawler_engine.crawl_multiple_sites(site_configs, max_articles_per_site=1, concurrent_sites=2)

            assert result["sites_crawled"] == 2
            assert result["total_articles"] == 2
            assert mock_crawl_site.call_count == 2

    @pytest.mark.asyncio
    async def test_crawl_multiple_sites_with_duplicates(self, crawler_engine, mock_site_config):
        """Test handling of duplicate articles in multi-site crawling"""
        site_configs = [mock_site_config]
        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]

        with patch.object(crawler_engine, 'crawl_site', return_value=mock_articles) as _mock_crawl_site, \
             patch.object(crawler_engine, '_ingest_articles') as mock_ingest, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            mock_ingest.return_value = {
                'new_articles': 0,
                'duplicates': 1,
                'errors': 0,
                'details': [{'url': 'https://testsite.com/article', 'status': 'duplicate'}]
            }

            result = await crawler_engine.crawl_multiple_sites(site_configs, max_articles_per_site=1)

            assert result["total_articles"] == 0
            assert result["duplicates_skipped"] == 1
            assert result["site_duplicate_breakdown"]["testsite.com"] == 1

    @pytest.mark.asyncio
    async def test_crawl_multiple_sites_global_target(self, crawler_engine, mock_site_config):
        """Test the global target stops the crawl across multiple sites"""
        # Two sites, but global target is 1 so only one article should be ingested
        site_configs = [mock_site_config, SiteConfig({'id': 2, 'name': 'Other', 'domain': 'other.com', 'url': 'https://other.com'})]

        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]

        with patch.object(crawler_engine, 'crawl_site', return_value=mock_articles) as _mock_crawl_site, \
             patch.object(crawler_engine, '_ingest_articles') as mock_ingest, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            # First site ingests 1 article, second site should be skipped due to global target
            mock_ingest.side_effect = [
                {'new_articles': 1, 'duplicates': 0, 'errors': 0, 'details': [{'url': 'https://testsite.com/article', 'status': 'new'}]},
                {'new_articles': 1, 'duplicates': 0, 'errors': 0, 'details': [{'url': 'https://other.com/article', 'status': 'new'}]},
            ]

            result = await crawler_engine.crawl_multiple_sites(site_configs, max_articles_per_site=5, concurrent_sites=1, global_target_total=1)

            assert result["total_articles"] == 1
            assert result["global_target_total"] == 1
            assert result["global_target_reached"] is True

    @pytest.mark.asyncio
    async def test_crawl_multiple_sites_global_target_concurrent_reservation(self, crawler_engine):
        """Test concurrent sites respect the global reservation so we don't overshoot the target"""
        site_configs = [
            SiteConfig({'id': 1, 'name': 'Site 1', 'domain': 'site1.com', 'url': 'https://site1.com'}),
            SiteConfig({'id': 2, 'name': 'Site 2', 'domain': 'site2.com', 'url': 'https://site2.com'}),
        ]

        # Both sites return two candidates but global target is 2 - reservation should ensure only one
        # ingestion call (or at most the necessary number of new articles) satisfies the target.
        mock_articles = [
            {"title": "Article A", "url": "https://site1.com/a", "content": "C"},
            {"title": "Article B", "url": "https://site1.com/b", "content": "C"},
        ]

        with patch.object(crawler_engine, 'crawl_site', return_value=mock_articles) as _mock_crawl_site, \
             patch.object(crawler_engine, '_ingest_articles') as mock_ingest, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            # only first call should be required to hit the global target
            mock_ingest.side_effect = [
                {'new_articles': 2, 'duplicates': 0, 'errors': 0, 'details': [{'url': 'https://site1.com/a', 'status': 'new'}]},
            ]

            result = await crawler_engine.crawl_multiple_sites(site_configs, max_articles_per_site=5, concurrent_sites=2, global_target_total=2)

            assert result["total_articles"] == 2
            # ensure we didn't call ingest twice (second site should have been skipped)
            assert mock_ingest.call_count == 1

    @pytest.mark.asyncio
    async def test_crawl_multiple_sites_global_target_reservation_with_duplicates(self, crawler_engine):
        """If first site reserves 2 but ingests only 1 (duplicate), the remaining allowance should be returned and used by others."""
        site_configs = [
            SiteConfig({'id': 1, 'name': 'Site 1', 'domain': 'site1.com', 'url': 'https://site1.com'}),
            SiteConfig({'id': 2, 'name': 'Site 2', 'domain': 'site2.com', 'url': 'https://site2.com'}),
        ]

        mock_articles = [
            {"title": "Article A", "url": "https://site1.com/a", "content": "C"},
            {"title": "Article B", "url": "https://site1.com/b", "content": "C"},
        ]

        with patch.object(crawler_engine, 'crawl_site', return_value=mock_articles) as _mock_crawl_site, \
             patch.object(crawler_engine, '_ingest_articles') as mock_ingest, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            # First reservation will be for 2 but only 1 will be ingested (duplicate). The leftover should be
            # returned to the global remaining and used by the second site.
            mock_ingest.side_effect = [
                {'new_articles': 1, 'duplicates': 1, 'errors': 0, 'details': [{'url': 'https://site1.com/a', 'status': 'new'}]},
                {'new_articles': 1, 'duplicates': 0, 'errors': 0, 'details': [{'url': 'https://site2.com/a', 'status': 'new'}]},
            ]

            result = await crawler_engine.crawl_multiple_sites(site_configs, max_articles_per_site=5, concurrent_sites=2, global_target_total=2)

            # Due to timing/concurrency the leftover allowance may or may not be
            # picked up by other concurrent site tasks; ensure we never exceed
            # the requested global capacity and we ingest at least one article.
            assert 1 <= result["total_articles"] <= 2
            assert mock_ingest.call_count in (1, 2)

    @pytest.mark.asyncio
    async def test_ingest_articles_success(self, crawler_engine):
        """Test successful article ingestion via MCP bus"""
        articles = [
            {
                "url": "https://testsite.com/article1",
                "title": "Test Article 1",
                "content": "Content 1",
                "domain": "testsite.com"
            },
            {
                "url": "https://testsite.com/article2",
                "title": "Test Article 2",
                "content": "Content 2",
                "domain": "testsite.com"
            }
        ]

        mock_response = Mock()
        mock_response.json.return_value = {
            "status": "ok",
            "data": {"status": "ok", "duplicate": False}
        }
        mock_response.raise_for_status.return_value = None

        with patch('agents.crawler.crawler_engine.requests.post', return_value=mock_response) as mock_post, \
             patch('agents.crawler.crawler_engine.make_json_safe', side_effect=lambda x: x):

            result = await crawler_engine._ingest_articles(articles)

            assert result["new_articles"] == 2
            assert result["duplicates"] == 0
            assert result["errors"] == 0
            assert len(result["details"]) == 2

            # Verify MCP bus calls
            assert mock_post.call_count == 2
            for call in mock_post.call_args_list:
                args, kwargs = call
                assert args[0] == "http://localhost:8000/call"
                payload = json.loads(kwargs["data"])
                assert payload["agent"] == "memory"
                assert payload["tool"] == "ingest_article"

    @pytest.mark.asyncio
    async def test_ingest_articles_with_duplicates(self, crawler_engine):
        """Test article ingestion with duplicate detection"""
        articles = [
            {
                "url": "https://testsite.com/article1",
                "title": "Test Article 1",
                "content": "Content 1",
                "domain": "testsite.com"
            }
        ]

        mock_response = Mock()
        mock_response.json.return_value = {
            "status": "ok",
            "data": {"status": "ok", "duplicate": True}
        }
        mock_response.raise_for_status.return_value = None

        with patch('agents.crawler.crawler_engine.requests.post', return_value=mock_response) as _mock_post, \
             patch('agents.crawler.crawler_engine.make_json_safe', side_effect=lambda x: x):

            result = await crawler_engine._ingest_articles(articles)

            assert result["new_articles"] == 0
            assert result["duplicates"] == 1
            assert result["errors"] == 0
            assert result["details"][0]["status"] == "duplicate"

    @pytest.mark.asyncio
    async def test_ingest_articles_error_handling(self, crawler_engine):
        """Test article ingestion error handling"""
        articles = [
            {
                "url": "https://testsite.com/article1",
                "title": "Test Article 1",
                "content": "Content 1",
                "domain": "testsite.com"
            }
        ]

        with patch('agents.crawler.crawler_engine.requests.post', side_effect=Exception("Network error")) as _mock_post, \
             patch('agents.crawler.crawler_engine.make_json_safe', side_effect=lambda x: x):

            result = await crawler_engine._ingest_articles(articles)

            assert result["new_articles"] == 0
            assert result["duplicates"] == 0
            assert result["errors"] == 1
            assert result["details"][0]["status"] == "error"
            assert "Network error" in result["details"][0]["error"]

    @pytest.mark.asyncio
    async def test_run_unified_crawl_basic(self, crawler_engine):
        """Test run_unified_crawl main entry point"""
        domains = ["testsite.com"]
        mock_articles = [
            {"title": "Test Article", "url": "https://testsite.com/article", "content": "Content"}
        ]

        with patch('agents.crawler.crawler_engine.get_sources_by_domain', return_value=[
            {"id": 1, "name": "Test Site", "domain": "testsite.com", "url": "https://testsite.com"}
        ]) as mock_get_sources, \
             patch.object(crawler_engine, 'crawl_multiple_sites') as mock_crawl_multiple:

            mock_crawl_multiple.return_value = {
                "unified_crawl": True,
                "sites_crawled": 1,
                "total_articles": 1,
                "articles": mock_articles
            }

            result = await crawler_engine.run_unified_crawl(domains, max_articles_per_site=1)

            assert result["unified_crawl"] is True
            assert result["sites_crawled"] == 1
            mock_get_sources.assert_called_once_with(["testsite.com"])
            mock_crawl_multiple.assert_called_once()

    @pytest.mark.asyncio
    async def test_run_unified_crawl_unknown_domain(self, crawler_engine):
        """Test run_unified_crawl with unknown domain creates basic config"""
        domains = ["unknownsite.com"]

        with patch('agents.crawler.crawler_engine.get_sources_by_domain', return_value=[]) as mock_get_sources, \
             patch.object(crawler_engine, 'crawl_multiple_sites') as mock_crawl_multiple:

            mock_crawl_multiple.return_value = {
                "unified_crawl": True,
                "sites_crawled": 1,
                "total_articles": 0,
                "articles": []
            }

            result = await crawler_engine.run_unified_crawl(domains, max_articles_per_site=1)

            assert result["unified_crawl"] is True
            mock_get_sources.assert_called_once_with(["unknownsite.com"])
            mock_crawl_multiple.assert_called_once()

            # Verify SiteConfig was created with basic config
            call_args = mock_crawl_multiple.call_args
            site_configs = call_args[0][0]
            assert len(site_configs) == 1
            config = site_configs[0]
            assert config.domain == "unknownsite.com"
            assert config.url == "https://unknownsite.com"

    @pytest.mark.asyncio
    async def test_run_unified_crawl_global_target_forwarding(self, crawler_engine):
        """Test that run_unified_crawl forwards global_target_total to crawl_multiple_sites"""
        domains = ["testsite.com"]
        with patch.object(crawler_engine, 'crawl_multiple_sites') as mock_crawl_multiple:
            mock_crawl_multiple.return_value = {
                "unified_crawl": True,
                "sites_crawled": 1,
                "total_articles": 0,
                "articles": []
            }

            result = await crawler_engine.run_unified_crawl(domains, max_articles_per_site=5, global_target_total=10)
            mock_crawl_multiple.assert_called_once()
            # Check that the forwarded kwarg exists on the call
            _, kwargs = mock_crawl_multiple.call_args
            assert 'global_target_total' in kwargs and kwargs['global_target_total'] == 10

    def test_execute_crawl_forwards_global_target(self):
        """execute_crawl should forward the global_target_total argument into the async runner"""
        with patch('agents.crawler.main.CrawlerEngine') as mock_engine_class:
            mock_engine = mock_engine_class.return_value.__aenter__.return_value
            async def fake_run(domains, max_articles, concurrent, global_target_total=None, profile_overrides=None):
                return {'unified_crawl': True, 'total_articles': 0, 'global_target_total': global_target_total}

            mock_engine.run_unified_crawl = AsyncMock(side_effect=fake_run)

            res = execute_crawl(['example.com'], max_articles_per_site=5, concurrent_sites=1, profile_overrides=None, global_target_total=7)
            assert res.get('global_target_total') == 7

    @pytest.mark.asyncio
    async def test_run_unified_crawl_no_valid_domains(self, crawler_engine):
        """Test run_unified_crawl with no valid domains"""
        domains = []

        result = await crawler_engine.run_unified_crawl(domains)

        assert result == {"error": "No valid domains provided"}

    @pytest.mark.asyncio
    async def test_get_performance_report(self, crawler_engine):
        """Test performance report generation"""
        # Simulate some activity
        crawler_engine.performance_metrics["articles_processed"] = 42
        crawler_engine.performance_metrics["sites_crawled"] = 5
        crawler_engine.performance_metrics["errors"] = 2
        crawler_engine.performance_metrics["mode_usage"]["ultra_fast"] = 3
        crawler_engine.performance_metrics["mode_usage"]["generic"] = 2

        report = crawler_engine.get_performance_report()

        assert report["articles_processed"] == 42
        assert report["sites_crawled"] == 5
        assert report["errors"] == 2
        assert report["mode_usage"]["ultra_fast"] == 3
        assert report["mode_usage"]["generic"] == 2
        assert "uptime_seconds" in report
        assert report["uptime_seconds"] >= 0

    @pytest.mark.asyncio
    async def test_error_handling_crawl_failure(self, crawler_engine, mock_site_config):
        """Test error handling when crawling fails"""
        with patch.object(crawler_engine, 'crawl_site', side_effect=Exception("Crawl failed")) as _mock_crawl_site, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            result = await crawler_engine.crawl_multiple_sites([mock_site_config], max_articles_per_site=1)

            # Should still return valid structure despite errors
            assert result["unified_crawl"] is True
            assert result["sites_crawled"] == 1
            assert result["total_articles"] == 0
            assert result["ingestion_errors"] == 1  # Crawl failure is counted as ingestion error
            assert result["site_error_breakdown"]["testsite.com"] == 1

            # Performance metrics should reflect the error
            assert crawler_engine.performance_metrics["errors"] == 1

    @pytest.mark.asyncio
    async def test_cleanup_orphaned_processes(self, crawler_engine):
        """Test orphaned process cleanup functionality"""
        with patch('subprocess.run') as mock_subprocess, \
             patch('os.kill') as mock_kill, \
             patch('time.sleep'):  # Prevent actual sleep

            # Mock pgrep finding processes and ps showing old processes
            def mock_run(*args, **kwargs):
                if 'pgrep' in str(args[0]):
                    result = Mock()
                    result.returncode = 0
                    result.stdout = "1234\n5678\n"
                    return result
                elif 'ps' in str(args[0]):
                    result = Mock()
                    result.returncode = 0
                    result.stdout = "700"  # 700 seconds = 11+ minutes
                    return result
                return Mock(returncode=1)

            mock_subprocess.side_effect = mock_run

            await crawler_engine._cleanup_orphaned_processes()

            # Should have attempted to kill old processes
            assert mock_kill.call_count >= 2  # At least Chrome and Playwright processes

    @pytest.mark.asyncio
    async def test_determine_strategy_with_performance_history(self, crawler_engine, mock_site_config):
        """Test strategy determination using performance history"""
        # Mock performance history with good ultra_fast performance
        crawler_engine.performance_history[mock_site_config.domain] = [
            {"strategy_used": "ultra_fast", "articles_per_second": 10.0},
            {"strategy_used": "generic", "articles_per_second": 2.0}
        ]

        with patch('agents.crawler.crawler_engine.get_source_performance_history', return_value=[
            {"strategy_used": "ultra_fast", "articles_per_second": 10.0},
            {"strategy_used": "generic", "articles_per_second": 2.0}
        ]):

            strategy = await crawler_engine._determine_optimal_strategy(mock_site_config)

            # Should choose ultra_fast due to better performance
            assert strategy == "ultra_fast"
            assert crawler_engine.strategy_cache[f"{mock_site_config.domain}_{mock_site_config.source_id}"] == "ultra_fast"

    @pytest.mark.asyncio
    async def test_crawl_with_profile_unknown_engine(self, crawler_engine, mock_site_config):
        """Test crawling with unknown profile engine falls back to generic"""
        profile = {"engine": "unknown_engine"}

        with patch.object(crawler_engine, '_crawl_generic_mode') as mock_generic, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            await crawler_engine._crawl_with_profile(mock_site_config, profile, 5)

            mock_generic.assert_called_once_with(mock_site_config, 5)

    @pytest.mark.asyncio
    async def test_crawl_generic_mode_error_handling(self, crawler_engine, mock_site_config):
        """Test generic mode handles crawling errors gracefully"""
        with patch('agents.crawler.crawler_engine.GenericSiteCrawler') as mock_crawler_class, \
             patch.object(crawler_engine, '_cleanup_orphaned_processes'):

            mock_crawler = Mock()
            mock_crawler.crawl_site.side_effect = Exception("Crawling failed")
            mock_crawler_class.return_value = mock_crawler

            result = await crawler_engine._crawl_generic_mode(mock_site_config, max_articles=5)

            assert result == []  # Should return empty list on error
            # Performance metrics are not updated on error
            assert crawler_engine.performance_metrics["mode_usage"]["generic"] == 0
