"""
Integration tests for Analyst + Fact-Checker integration.

Tests the mandatory per-article fact-checking feature as specified in
docs/feat_article_creation.md.
"""

from unittest.mock import patch

import pytest


@pytest.fixture
def fake_engine():
    """Provide a real AnalystEngine instance for integration tests."""
    from agents.analyst.analyst_engine import AnalystEngine

    return AnalystEngine()


@pytest.fixture
def mock_fact_checker():
    """Mock comprehensive_fact_check from fact-checker tools."""
    with patch("agents.fact_checker.tools.comprehensive_fact_check") as mock:
        # Default successful fact-check response
        mock.return_value = {
            "overall_score": 0.85,
            "fact_verification": {
                "verification_score": 0.82,
                "classification": "mostly_true",
                "confidence": 0.9,
                "claims_analyzed": 3,
            },
            "credibility_assessment": {
                "credibility_score": 0.88,
                "reliability": "high",
                "domain_score": 0.9,
            },
            "claims_analysis": {
                "claims": [
                    {"text": "Test claim 1", "verdict": "verified", "confidence": 0.9},
                    {"text": "Test claim 2", "verdict": "verified", "confidence": 0.85},
                ],
                "claim_count": 2,
            },
            "contradictions_analysis": {
                "contradictions_found": False,
                "count": 0,
            },
            "processing_timestamp": "2024-01-01T00:00:00Z",
        }
        yield mock


class TestFactCheckerIntegration:
    """Test Analyst integration with Fact-Checker."""

    def test_generate_analysis_report_calls_fact_checker(
        self, fake_engine, mock_fact_checker
    ):
        """Test that generate_analysis_report calls fact-checker for each article."""
        texts = ["Article 1 text", "Article 2 text"]
        article_ids = ["art1", "art2"]

        _report = fake_engine.generate_analysis_report(
            texts=texts,
            article_ids=article_ids,
            cluster_id="test_cluster",
            enable_fact_check=True,
        )

        # Verify fact-checker was called for each article
        assert mock_fact_checker.call_count == 2

        # Verify call arguments
        first_call = mock_fact_checker.call_args_list[0]
        assert first_call[1]["content"] == "Article 1 text"
        assert first_call[1]["source_url"] == "art1"

        second_call = mock_fact_checker.call_args_list[1]
        assert second_call[1]["content"] == "Article 2 text"
        assert second_call[1]["source_url"] == "art2"

    def test_generate_analysis_report_attaches_source_fact_checks(
        self, fake_engine, mock_fact_checker
    ):
        """Test that source_fact_checks are attached to AnalysisReport."""
        texts = ["Article text"]
        article_ids = ["art1"]

        report = fake_engine.generate_analysis_report(
            texts=texts, article_ids=article_ids, enable_fact_check=True
        )

        # Verify source_fact_checks field exists
        assert "source_fact_checks" in report
        assert len(report["source_fact_checks"]) == 1

        sfc = report["source_fact_checks"][0]
        assert sfc["article_id"] == "art1"
        assert sfc["fact_check_status"] == "passed"  # overall_score=0.85 >= 0.8
        assert sfc["overall_score"] == 0.85
        assert sfc["credibility_score"] == 0.88

    def test_generate_analysis_report_per_article_includes_fact_check(
        self, fake_engine, mock_fact_checker
    ):
        """Test that per_article results include source_fact_check."""
        texts = ["Article text"]
        article_ids = ["art1"]

        report = fake_engine.generate_analysis_report(
            texts=texts, article_ids=article_ids, enable_fact_check=True
        )

        per_article = report["per_article"][0]
        assert per_article["source_fact_check"] is not None
        assert per_article["source_fact_check"]["article_id"] == "art1"
        assert per_article["source_fact_check"]["fact_check_status"] == "passed"

    def test_generate_analysis_report_cluster_summary(
        self, fake_engine, mock_fact_checker
    ):
        """Test cluster_fact_check_summary aggregation."""
        # Mock different fact-check statuses
        mock_fact_checker.side_effect = [
            {  # Article 1: passed (score=0.85)
                "overall_score": 0.85,
                "fact_verification": {
                    "verification_score": 0.85,
                    "classification": "verified",
                    "confidence": 0.9,
                    "claims_analyzed": 2,
                },
                "credibility_assessment": {
                    "credibility_score": 0.88,
                    "reliability": "high",
                },
                "claims_analysis": {"claims": [], "claim_count": 0},
                "contradictions_analysis": {"contradictions_found": False},
                "processing_timestamp": "2024-01-01T00:00:00Z",
            },
            {  # Article 2: needs_review (score=0.65)
                "overall_score": 0.65,
                "fact_verification": {
                    "verification_score": 0.65,
                    "classification": "questionable",
                    "confidence": 0.7,
                    "claims_analyzed": 1,
                },
                "credibility_assessment": {
                    "credibility_score": 0.70,
                    "reliability": "medium",
                },
                "claims_analysis": {"claims": [], "claim_count": 0},
                "contradictions_analysis": {"contradictions_found": False},
                "processing_timestamp": "2024-01-01T00:00:01Z",
            },
            {  # Article 3: failed (score=0.45)
                "overall_score": 0.45,
                "fact_verification": {
                    "verification_score": 0.45,
                    "classification": "false",
                    "confidence": 0.8,
                    "claims_analyzed": 3,
                },
                "credibility_assessment": {
                    "credibility_score": 0.50,
                    "reliability": "low",
                },
                "claims_analysis": {"claims": [], "claim_count": 0},
                "contradictions_analysis": {"contradictions_found": True, "count": 2},
                "processing_timestamp": "2024-01-01T00:00:02Z",
            },
        ]

        texts = ["Article 1", "Article 2", "Article 3"]
        article_ids = ["art1", "art2", "art3"]

        report = fake_engine.generate_analysis_report(
            texts=texts,
            article_ids=article_ids,
            cluster_id="test_cluster",
            enable_fact_check=True,
        )

        summary = report["cluster_fact_check_summary"]
        assert summary["total_articles_checked"] == 3
        assert summary["passed_count"] == 1
        assert summary["failed_count"] == 1
        assert summary["needs_review_count"] == 1
        assert summary["average_overall_score"] == pytest.approx(
            (0.85 + 0.65 + 0.45) / 3, rel=1e-3
        )
        assert summary["articles_flagged"] == ["art3"]
        assert summary["percent_verified"] == pytest.approx(100.0 / 3, rel=1e-2)

    def test_fact_check_status_thresholds(self, fake_engine, mock_fact_checker):
        """Test fact_check_status is correctly determined by overall_score."""
        test_cases = [
            (0.95, "passed"),
            (0.80, "passed"),
            (0.75, "needs_review"),
            (0.60, "needs_review"),
            (0.55, "failed"),
            (0.20, "failed"),
        ]

        for score, expected_status in test_cases:
            mock_fact_checker.reset_mock()
            mock_fact_checker.return_value = {
                "overall_score": score,
                "fact_verification": {
                    "verification_score": score,
                    "classification": "test",
                    "confidence": 0.9,
                    "claims_analyzed": 1,
                },
                "credibility_assessment": {
                    "credibility_score": score,
                    "reliability": "test",
                },
                "claims_analysis": {"claims": [], "claim_count": 0},
                "contradictions_analysis": {"contradictions_found": False},
                "processing_timestamp": "2024-01-01T00:00:00Z",
            }

            report = fake_engine.generate_analysis_report(
                texts=["Test text"], article_ids=["test"], enable_fact_check=True
            )

            sfc = report["source_fact_checks"][0]
            assert sfc["fact_check_status"] == expected_status, (
                f"Score {score} should map to status {expected_status}"
            )

    def test_generate_analysis_report_disable_fact_check(
        self, fake_engine, mock_fact_checker
    ):
        """Test that fact-checking can be disabled with enable_fact_check=False."""
        report = fake_engine.generate_analysis_report(
            texts=["Article text"], article_ids=["art1"], enable_fact_check=False
        )

        # Verify fact-checker was not called
        assert mock_fact_checker.call_count == 0

        # Verify no source_fact_checks
        assert report["source_fact_checks"] == []
        assert report["cluster_fact_check_summary"] is None

        # Verify per_article has no source_fact_check
        per_article = report["per_article"][0]
        assert per_article["source_fact_check"] is None

    def test_fact_checker_import_error_handling(self, fake_engine, caplog):
        """Test graceful handling when fact-checker is not available."""
        with patch(
            "agents.fact_checker.tools.comprehensive_fact_check",
            side_effect=ImportError("No module named 'agents.fact_checker'"),
        ):
            report = fake_engine.generate_analysis_report(
                texts=["Article text"], article_ids=["art1"], enable_fact_check=True
            )

            # Should not crash, just log error
            assert (
                "Per-article fact-check failed for art1" in caplog.text
                or "Fact-checker tools not available" in caplog.text
            )
            assert report["source_fact_checks"] == []

    def test_fact_checker_error_response_handling(
        self, fake_engine, mock_fact_checker, caplog
    ):
        """Test handling of fact-checker error responses."""
        mock_fact_checker.return_value = {"error": "Fact-check service unavailable"}

        report = fake_engine.generate_analysis_report(
            texts=["Article text"], article_ids=["art1"], enable_fact_check=True
        )

        # Should log error and continue
        assert "Fact-check failed for article" in caplog.text
        assert len(report["source_fact_checks"]) == 0

    def test_claim_verdicts_extraction(self, fake_engine, mock_fact_checker):
        """Test that claim_verdicts are correctly extracted from fact-checker response."""
        mock_fact_checker.return_value = {
            "overall_score": 0.80,
            "fact_verification": {
                "verification_score": 0.80,
                "classification": "verified",
                "confidence": 0.9,
                "claims_analyzed": 3,
            },
            "credibility_assessment": {
                "credibility_score": 0.85,
                "reliability": "high",
            },
            "claims_analysis": {
                "claims": [
                    {
                        "text": "Claim 1",
                        "verdict": "verified",
                        "confidence": 0.95,
                        "evidence": [{"source": "test"}],
                    },
                    {
                        "text": "Claim 2",
                        "verdict": "questionable",
                        "confidence": 0.70,
                        "evidence": None,
                    },
                    {
                        "text": "Claim 3",
                        "verdict": "false",
                        "confidence": 0.85,
                        "evidence": [],
                    },
                ],
                "claim_count": 3,
            },
            "contradictions_analysis": {"contradictions_found": False},
            "processing_timestamp": "2024-01-01T00:00:00Z",
        }

        report = fake_engine.generate_analysis_report(
            texts=["Article text"], article_ids=["art1"], enable_fact_check=True
        )

        sfc = report["source_fact_checks"][0]
        claim_verdicts = sfc["claim_verdicts"]

        assert len(claim_verdicts) == 3
        assert claim_verdicts[0]["claim_text"] == "Claim 1"
        assert claim_verdicts[0]["verdict"] == "verified"
        assert claim_verdicts[0]["confidence"] == 0.95
        assert claim_verdicts[0]["evidence"] == [{"source": "test"}]

        assert claim_verdicts[1]["claim_text"] == "Claim 2"
        assert claim_verdicts[1]["verdict"] == "questionable"
        assert claim_verdicts[1]["confidence"] == 0.70

    def test_fact_check_trace_includes_full_details(
        self, fake_engine, mock_fact_checker
    ):
        """Test that fact_check_trace captures comprehensive details."""
        mock_fact_checker.return_value = {
            "overall_score": 0.75,
            "fact_verification": {
                "verification_score": 0.75,
                "classification": "mostly_true",
                "confidence": 0.88,
                "claims_analyzed": 5,
            },
            "credibility_assessment": {
                "credibility_score": 0.80,
                "reliability": "high",
                "domain_score": 0.85,
            },
            "claims_analysis": {
                "claims": [],
                "claim_count": 5,
            },
            "contradictions_analysis": {
                "contradictions_found": True,
                "count": 1,
                "details": "Minor contradiction detected",
            },
            "processing_timestamp": "2024-01-01T00:00:00Z",
        }

        report = fake_engine.generate_analysis_report(
            texts=["Article text"], article_ids=["art1"], enable_fact_check=True
        )

        sfc = report["source_fact_checks"][0]
        trace = sfc["fact_check_trace"]

        assert trace["fact_verification"]["verification_score"] == 0.75
        assert trace["credibility_assessment"]["credibility_score"] == 0.80
        assert trace["claims_analyzed"] == 5
        assert trace["contradictions"]["contradictions_found"] is True
        assert trace["contradictions"]["count"] == 1

    def test_empty_texts_list(self, fake_engine, mock_fact_checker):
        """Test handling of empty texts list."""
        report = fake_engine.generate_analysis_report(
            texts=[], article_ids=[], enable_fact_check=True
        )

        assert mock_fact_checker.call_count == 0
        assert report["source_fact_checks"] == []
        assert report["cluster_fact_check_summary"] is None
        assert report["articles_count"] == 0

    def test_multiple_articles_fact_check_isolation(
        self, fake_engine, mock_fact_checker
    ):
        """Test that each article gets independent fact-check call."""
        # Set up different responses for each article
        responses = [
            {
                "overall_score": 0.90,
                "fact_verification": {
                    "verification_score": 0.90,
                    "classification": "verified",
                    "confidence": 0.95,
                    "claims_analyzed": 2,
                },
                "credibility_assessment": {"credibility_score": 0.92},
                "claims_analysis": {"claims": [], "claim_count": 0},
                "contradictions_analysis": {"contradictions_found": False},
                "processing_timestamp": "2024-01-01T00:00:00Z",
            },
            {
                "overall_score": 0.50,
                "fact_verification": {
                    "verification_score": 0.50,
                    "classification": "questionable",
                    "confidence": 0.70,
                    "claims_analyzed": 1,
                },
                "credibility_assessment": {"credibility_score": 0.55},
                "claims_analysis": {"claims": [], "claim_count": 0},
                "contradictions_analysis": {"contradictions_found": True},
                "processing_timestamp": "2024-01-01T00:00:01Z",
            },
        ]
        mock_fact_checker.side_effect = responses

        report = fake_engine.generate_analysis_report(
            texts=["Article 1", "Article 2"],
            article_ids=["art1", "art2"],
            enable_fact_check=True,
        )

        # Verify independent results
        assert len(report["source_fact_checks"]) == 2
        assert report["source_fact_checks"][0]["overall_score"] == 0.90
        assert report["source_fact_checks"][0]["fact_check_status"] == "passed"
        assert report["source_fact_checks"][1]["overall_score"] == 0.50
        assert report["source_fact_checks"][1]["fact_check_status"] == "failed"
