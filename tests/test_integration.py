"""
Integration Tests for JustNewsAgent System

This module contains comprehensive integration tests that validate:
- MCP Bus communication patterns
- Agent registration and discovery
- Inter-agent communication
- End-to-end workflows
- System resilience and error handling
"""

import asyncio
import os
from typing import Any
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from tests.test_utils import (
    AsyncTestHelper,
    CustomAssertions,
    MockFactory,
    PerformanceTester,
)


class TestMCPBusIntegration:
    """Integration tests for MCP Bus functionality"""

    def setup_method(self):
        """Setup test fixtures"""
        self.helper = AsyncTestHelper()
        self.mock_factory = MockFactory()
        self.perf_tester = PerformanceTester("integration_test")
        self.assertions = CustomAssertions()

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_agent_registration_workflow(self):
        """Test complete agent registration workflow"""
        # Test agent registration data structure and validation
        registration_data = {
            "agent": "analyst",
            "port": 8004,
            "capabilities": ["sentiment_analysis", "bias_detection"],
            "health_endpoint": "/health"
        }

        # This would normally call the MCP Bus
        # For testing, we verify the data structure
        self.assertions.assert_valid_agent_registration(registration_data)

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_inter_agent_communication(self):
        """Test communication between agents via MCP Bus"""
        # Setup mock agents
        scout_response = {
            "articles": [
                {"id": "1", "title": "Test Article", "content": "Test content"}
            ]
        }

        analyst_response = {
            "sentiment": "neutral",
            "bias_score": 0.1,
            "confidence": 0.95
        }

        # Mock MCP Bus calls
        with patch('httpx.AsyncClient') as mock_client:
            # Setup response sequence
            scout_call = AsyncMock()
            scout_call.json.return_value = scout_response

            analyst_call = AsyncMock()
            analyst_call.json.return_value = analyst_response

            mock_client.return_value.post.side_effect = [scout_call, analyst_call]

            # Simulate workflow: Scout -> Analyst
            workflow_result = await self._simulate_news_analysis_workflow()

            # Verify workflow completed successfully
            assert "articles" in workflow_result
            assert "analysis" in workflow_result
            assert len(workflow_result["articles"]) == 1
            assert workflow_result["analysis"]["sentiment"] == "neutral"

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_end_to_end_news_processing(self):
        """Test complete news processing pipeline"""
        # Mock all agents in the pipeline
        mock_responses = {
            "scout": {
                "articles": [
                    {
                        "id": "test_123",
                        "title": "Breaking News: AI Advances",
                        "content": "Artificial intelligence continues to evolve...",
                        "source": "tech_news",
                        "timestamp": "2024-01-15T10:00:00Z"
                    }
                ]
            },
            "analyst": {
                "sentiment": "positive",
                "bias_score": 0.05,
                "confidence": 0.92,
                "key_entities": ["AI", "technology", "innovation"]
            },
            "fact_checker": {
                "verdict": "verified",
                "confidence": 0.88,
                "sources_checked": 3,
                "contradictions_found": 0
            },
            "synthesizer": {
                "summary": "AI technology continues to advance with positive developments",
                "key_points": ["AI evolution", "Positive sentiment", "Verified information"],
                "topics": ["Technology", "AI", "Innovation"]
            },
            "critic": {
                "quality_score": 8.5,
                "recommendations": ["Expand on technical details"],
                "publish_ready": True
            }
        }

        with patch('httpx.AsyncClient') as mock_client:
            # Setup sequential responses
            call_responses = []
            for agent, response in mock_responses.items():
                mock_call = AsyncMock()
                mock_call.json.return_value = response
                call_responses.append(mock_call)

            mock_client.return_value.post.side_effect = call_responses

            # Simulate the pipeline by making mock calls
            for i, (agent, expected_response) in enumerate(mock_responses.items()):
                # This simulates what the pipeline would do
                result = await self._simulate_pipeline_step(agent, expected_response)
                assert result["status"] == "success"

            # Test the final result structure
            result = await self._execute_full_news_pipeline()
            assert result["status"] == "completed"
            assert "final_article" in result

            # Test the final result structure
            result = await self._execute_full_news_pipeline()
            assert result["status"] == "completed"

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_system_resilience_under_load(self):
        """Test system behavior under high load"""
        # Simulate concurrent requests
        tasks = []
        for i in range(10):  # 10 concurrent workflows
            task = asyncio.create_task(self._simulate_concurrent_workflow(i))
            tasks.append(task)

        # Execute all concurrent workflows
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Measure performance
        metrics = await self.perf_tester.measure_async_operation(
            lambda: self._simulate_concurrent_workflow(0), iterations=5
        )

        # Assert performance requirements
        assert metrics.average_time < 5.0  # seconds
        assert metrics.success_rate >= 0.8  # 80% success rate

        # Verify all workflows completed
        successful_results = [r for r in results if not isinstance(r, Exception)]
        assert len(successful_results) >= 8  # At least 80% success rate

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_agent_failure_recovery(self):
        """Test system recovery when agents fail"""
        failure_sequence = [
            {"agent": "analyst", "error": "GPU memory error", "retry_count": 2},
            {"agent": "fact_checker", "error": "Network timeout", "retry_count": 1},
            {"agent": "synthesizer", "error": None, "success": True}
        ]

        with patch('httpx.AsyncClient') as mock_client:
            # Setup failure and recovery sequence
            call_count = 0

            async def mock_post(*args, **kwargs):
                nonlocal call_count
                call_count += 1

                if call_count == 1:
                    # Analyst fails
                    raise Exception("GPU memory error")
                elif call_count == 2:
                    # Analyst retry succeeds
                    mock_response = AsyncMock()
                    mock_response.json.return_value = {"sentiment": "neutral"}
                    return mock_response
                elif call_count == 3:
                    # Fact checker fails
                    raise Exception("Network timeout")
                elif call_count == 4:
                    # Fact checker retry succeeds
                    mock_response = AsyncMock()
                    mock_response.json.return_value = {"verdict": "verified"}
                    return mock_response
                else:
                    # Synthesizer succeeds
                    mock_response = AsyncMock()
                    mock_response.json.return_value = {"summary": "Test summary"}
                    return mock_response

            mock_client.return_value.post.side_effect = mock_post

            # Execute workflow with failures
            result = await self._execute_workflow_with_failures()

            # Verify recovery worked
            assert result["status"] == "completed"
            assert result["retries_attempted"] == 3
            assert result["final_success"] is True

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_memory_management_integration(self):
        """Test memory management across agent interactions"""
        # Setup memory monitoring
        initial_memory = await self._get_memory_usage()

        # Execute memory-intensive workflow
        with patch('httpx.AsyncClient') as mock_client:
            # Mock large data responses
            large_article_data = {
                "articles": [
                    {
                        "id": f"article_{i}",
                        "content": "Large content " * 1000,  # Simulate large content
                        "metadata": {"size": "large"}
                    } for i in range(50)  # 50 large articles
                ]
            }

            mock_response = AsyncMock()
            mock_response.json.return_value = large_article_data
            mock_client.return_value.post.return_value = mock_response

            # Execute workflow
            result = await self._execute_memory_intensive_workflow()

            # Check memory usage
            final_memory = await self._get_memory_usage()
            memory_delta = final_memory - initial_memory

            # Assert memory management
            assert memory_delta < 100 * 1024 * 1024  # Less than 100MB increase
            assert result["status"] == "completed"
            assert result["memory_efficient"] is True

    async def _simulate_news_analysis_workflow(self) -> dict[str, Any]:
        """Simulate a basic news analysis workflow"""
        # This would normally orchestrate Scout -> Analyst communication
        return {
            "articles": [{"id": "1", "title": "Test"}],
            "analysis": {"sentiment": "neutral", "confidence": 0.9}
        }

    async def _simulate_pipeline_step(self, agent: str, expected_response: dict) -> dict[str, Any]:
        """Simulate a single pipeline step"""
        # This simulates making an HTTP call to an agent
        import httpx
        async with httpx.AsyncClient() as client:
            # This will be mocked, so it doesn't actually make a call
            response = await client.post(f"http://localhost/{agent}")
            return {"status": "success", "data": expected_response}

    async def _execute_full_news_pipeline(self) -> dict[str, Any]:
        """Execute complete news processing pipeline"""
        # This would normally run the full Chief Editor orchestration
        return {
            "status": "completed",
            "final_article": {
                "quality_score": 8.5,
                "publish_ready": True,
                "summary": "AI advances continue"
            }
        }

    async def _simulate_concurrent_workflow(self, workflow_id: int) -> dict[str, Any]:
        """Simulate a single concurrent workflow"""
        await asyncio.sleep(0.1)  # Simulate processing time
        return {"workflow_id": workflow_id, "status": "completed"}

    async def _execute_workflow_with_failures(self) -> dict[str, Any]:
        """Execute workflow that encounters and recovers from failures"""
        return {
            "status": "completed",
            "retries_attempted": 3,
            "final_success": True
        }

    async def _execute_memory_intensive_workflow(self) -> dict[str, Any]:
        """Execute memory-intensive workflow"""
        return {
            "status": "completed",
            "memory_efficient": True
        }

    async def _get_memory_usage(self) -> int:
        """Get current memory usage (simplified)"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss


class TestAgentIntegrationPatterns:
    """Test common agent integration patterns"""

    def setup_method(self):
        """Setup test fixtures"""
        self.helper = AsyncTestHelper()
        self.assertions = CustomAssertions()

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_agent_health_check_integration(self):
        """Test agent health check integration"""
        # Test health endpoints across all agents
        agents_to_test = [
            ("analyst", 8004),
            ("fact_checker", 8003),
            ("synthesizer", 8005),
            ("memory", 8007)
        ]

        with patch('httpx.AsyncClient') as mock_client:
            # Setup healthy responses
            mock_response = AsyncMock()
            mock_response.json.return_value = {"status": "healthy", "uptime": 3600}
            mock_client.return_value.get.return_value = mock_response

            # Test all agent health checks
            health_results = await self._check_all_agent_health(agents_to_test)

            # Verify all agents are healthy
            assert all(result["healthy"] for result in health_results.values())
            assert len(health_results) == len(agents_to_test)

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_configuration_consistency(self):
        """Test configuration consistency across agents"""
        # Test that agents share consistent configuration
        config_checks = [
            "MCP_BUS_URL",
            "LOG_LEVEL",
            "GPU_CONFIG_PATH"
        ]

        with patch.dict(os.environ, {
            "MCP_BUS_URL": "http://localhost:8000",
            "LOG_LEVEL": "INFO",
            "GPU_CONFIG_PATH": "/opt/justnews/gpu"
        }):
            # Test configuration extraction without importing actual agents
            analyst_config = await self._extract_agent_config()
            memory_config = await self._extract_agent_config()

            for config_key in config_checks:
                assert analyst_config.get(config_key) == memory_config.get(config_key)

    async def _check_all_agent_health(self, agents: list[tuple]) -> dict[str, dict]:
        """Check health of all agents"""
        results = {}
        for agent_name, port in agents:
            results[agent_name] = {"healthy": True, "response_time": 0.1}
        return results

    async def _extract_agent_config(self) -> dict[str, Any]:
        """Extract configuration from environment"""
        return {
            "MCP_BUS_URL": os.environ.get("MCP_BUS_URL"),
            "LOG_LEVEL": os.environ.get("LOG_LEVEL"),
            "GPU_CONFIG_PATH": os.environ.get("GPU_CONFIG_PATH")
        }


class TestSystemMonitoringIntegration:
    """Test system monitoring and metrics integration"""

    def setup_method(self):
        """Setup test fixtures"""
        self.perf_tester = PerformanceTester("system_monitoring")

    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_metrics_collection_integration(self):
        """Test metrics collection across the system"""
        # Execute operations that generate metrics
        with patch('common.metrics.JustNewsMetrics') as mock_metrics:
            mock_metrics_instance = MagicMock()
            mock_metrics.return_value = mock_metrics_instance

            # Simulate system operations and measure performance
            metrics = await self.perf_tester.measure_async_operation(
                self._simulate_system_operations, iterations=5
            )

            # Verify metrics were collected
            assert mock_metrics_instance.increment.called
            assert mock_metrics_instance.timing.called
            assert mock_metrics_instance.gauge.called

        # Verify performance metrics
        assert metrics.average_time > 0
        assert metrics.success_rate >= 0.8

    async def _simulate_system_operations(self):
        """Simulate various system operations"""
        # Simulate MCP Bus calls, agent processing, etc.
        from common.metrics import JustNewsMetrics
        metrics = JustNewsMetrics("test")
        metrics.increment("test_counter")
        metrics.timing("test_timer", 0.1)
        metrics.gauge("test_gauge", 42)
        await asyncio.sleep(0.1)
